{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler  # Assuming you might need it\n",
    "\n",
    "# Specific models and tools\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Encoding and feature selection\n",
    "from category_encoders import TargetEncoder  # Fixed the import based on usage\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Miscellaneous settings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = [\n",
    "    'E0', \n",
    "    'E1', \n",
    "    \n",
    "    'E2', 'E3',\n",
    "        \n",
    "    'SC0', \n",
    "    'SC1',\n",
    "\n",
    "    'D1', \n",
    "    'D2',\n",
    "    'F1', \n",
    "    'F2',\n",
    "    'I1', \n",
    "    'I2',\n",
    "    'SP1', \n",
    "    'SP2',\n",
    "    'B1',\n",
    "    'G1',\n",
    "    'N1',\n",
    "    'P1',\n",
    "    'T1',\n",
    "]\n",
    "\n",
    "seasons = [\n",
    "    '2324', \n",
    "    '2223', '2122', '2021',\n",
    "    '1920', \n",
    "    #'1819', \n",
    "    #'1718', \n",
    "    #'1617',\n",
    "    #'1516', '1415', '1314', '1213',\n",
    "    #'1112', '1011', \n",
    "    #'0910', '0809',\n",
    "    #'0708', '0607', '0506', '0405',\n",
    "    #'0304', '0203', '0102', '0001',\n",
    "]\n",
    "\n",
    "countries = [\n",
    "    \"ARG\", \"AUT\", \"BRA\", \"CHN\",\n",
    "    \"DNK\", \"FIN\", \"IRL\", \"JPN\",\n",
    "    \"MEX\", \"NOR\", \"POL\", \"ROU\",\n",
    "    \"RUS\", \"SWE\", \"SWZ\", \"USA\",\n",
    "]\n",
    "\n",
    "fixtures = [\n",
    "    #\"fixtures\",\n",
    "    \"new_league_fixtures\"\n",
    "]\n",
    "\n",
    "content = \"euro_5s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all filepaths into a list\n",
    "matches_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in seasons:    \n",
    "    for comp in comps:  \n",
    "        matches_files.append('data/scraped/%s/%s.csv' % (season, comp))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:    \n",
    "    #matches_files.append('data/scraped/other/%s.csv' % (country))\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fixture in fixtures:    \n",
    "    #matches_files.append('data/scraped/fixtures/%s.csv' % (fixture))\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/scraped/2324/E0.csv',\n",
       " 'data/scraped/2324/E1.csv',\n",
       " 'data/scraped/2324/E2.csv',\n",
       " 'data/scraped/2324/E3.csv',\n",
       " 'data/scraped/2324/SC0.csv',\n",
       " 'data/scraped/2324/SC1.csv',\n",
       " 'data/scraped/2324/D1.csv',\n",
       " 'data/scraped/2324/D2.csv',\n",
       " 'data/scraped/2324/F1.csv',\n",
       " 'data/scraped/2324/F2.csv',\n",
       " 'data/scraped/2324/I1.csv',\n",
       " 'data/scraped/2324/I2.csv',\n",
       " 'data/scraped/2324/SP1.csv',\n",
       " 'data/scraped/2324/SP2.csv',\n",
       " 'data/scraped/2324/B1.csv',\n",
       " 'data/scraped/2324/G1.csv',\n",
       " 'data/scraped/2324/N1.csv',\n",
       " 'data/scraped/2324/P1.csv',\n",
       " 'data/scraped/2324/T1.csv',\n",
       " 'data/scraped/2223/E0.csv',\n",
       " 'data/scraped/2223/E1.csv',\n",
       " 'data/scraped/2223/E2.csv',\n",
       " 'data/scraped/2223/E3.csv',\n",
       " 'data/scraped/2223/SC0.csv',\n",
       " 'data/scraped/2223/SC1.csv',\n",
       " 'data/scraped/2223/D1.csv',\n",
       " 'data/scraped/2223/D2.csv',\n",
       " 'data/scraped/2223/F1.csv',\n",
       " 'data/scraped/2223/F2.csv',\n",
       " 'data/scraped/2223/I1.csv',\n",
       " 'data/scraped/2223/I2.csv',\n",
       " 'data/scraped/2223/SP1.csv',\n",
       " 'data/scraped/2223/SP2.csv',\n",
       " 'data/scraped/2223/B1.csv',\n",
       " 'data/scraped/2223/G1.csv',\n",
       " 'data/scraped/2223/N1.csv',\n",
       " 'data/scraped/2223/P1.csv',\n",
       " 'data/scraped/2223/T1.csv',\n",
       " 'data/scraped/2122/E0.csv',\n",
       " 'data/scraped/2122/E1.csv',\n",
       " 'data/scraped/2122/E2.csv',\n",
       " 'data/scraped/2122/E3.csv',\n",
       " 'data/scraped/2122/SC0.csv',\n",
       " 'data/scraped/2122/SC1.csv',\n",
       " 'data/scraped/2122/D1.csv',\n",
       " 'data/scraped/2122/D2.csv',\n",
       " 'data/scraped/2122/F1.csv',\n",
       " 'data/scraped/2122/F2.csv',\n",
       " 'data/scraped/2122/I1.csv',\n",
       " 'data/scraped/2122/I2.csv',\n",
       " 'data/scraped/2122/SP1.csv',\n",
       " 'data/scraped/2122/SP2.csv',\n",
       " 'data/scraped/2122/B1.csv',\n",
       " 'data/scraped/2122/G1.csv',\n",
       " 'data/scraped/2122/N1.csv',\n",
       " 'data/scraped/2122/P1.csv',\n",
       " 'data/scraped/2122/T1.csv',\n",
       " 'data/scraped/2021/E0.csv',\n",
       " 'data/scraped/2021/E1.csv',\n",
       " 'data/scraped/2021/E2.csv',\n",
       " 'data/scraped/2021/E3.csv',\n",
       " 'data/scraped/2021/SC0.csv',\n",
       " 'data/scraped/2021/SC1.csv',\n",
       " 'data/scraped/2021/D1.csv',\n",
       " 'data/scraped/2021/D2.csv',\n",
       " 'data/scraped/2021/F1.csv',\n",
       " 'data/scraped/2021/F2.csv',\n",
       " 'data/scraped/2021/I1.csv',\n",
       " 'data/scraped/2021/I2.csv',\n",
       " 'data/scraped/2021/SP1.csv',\n",
       " 'data/scraped/2021/SP2.csv',\n",
       " 'data/scraped/2021/B1.csv',\n",
       " 'data/scraped/2021/G1.csv',\n",
       " 'data/scraped/2021/N1.csv',\n",
       " 'data/scraped/2021/P1.csv',\n",
       " 'data/scraped/2021/T1.csv',\n",
       " 'data/scraped/1920/E0.csv',\n",
       " 'data/scraped/1920/E1.csv',\n",
       " 'data/scraped/1920/E2.csv',\n",
       " 'data/scraped/1920/E3.csv',\n",
       " 'data/scraped/1920/SC0.csv',\n",
       " 'data/scraped/1920/SC1.csv',\n",
       " 'data/scraped/1920/D1.csv',\n",
       " 'data/scraped/1920/D2.csv',\n",
       " 'data/scraped/1920/F1.csv',\n",
       " 'data/scraped/1920/F2.csv',\n",
       " 'data/scraped/1920/I1.csv',\n",
       " 'data/scraped/1920/I2.csv',\n",
       " 'data/scraped/1920/SP1.csv',\n",
       " 'data/scraped/1920/SP2.csv',\n",
       " 'data/scraped/1920/B1.csv',\n",
       " 'data/scraped/1920/G1.csv',\n",
       " 'data/scraped/1920/N1.csv',\n",
       " 'data/scraped/1920/P1.csv',\n",
       " 'data/scraped/1920/T1.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 33004 matches\n"
     ]
    }
   ],
   "source": [
    "# Load and concatenate matches data into a single DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in matches_files:\n",
    "\n",
    "    try:\n",
    "        df_temp = pd.read_csv(file)\n",
    "        df = pd.concat([df, df_temp], ignore_index=True)\n",
    "    except:\n",
    "        # print an error message\n",
    "        print(f'Error: {file} not found')\n",
    "\n",
    "# print the amount of data loaded\n",
    "print(f\"Data loaded: {df.shape[0]} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns if they exist\n",
    "df.rename(columns={\n",
    "    'Country': 'Div',\n",
    "    'Home': 'HomeTeam',\n",
    "    'Away': 'AwayTeam',\n",
    "    'Res': 'FTR',\n",
    "\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where 'Div' is not in the list of competitions\n",
    "#df = df[df['Div'].isin(comps)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate column names\n",
    "print(df.columns[df.columns.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Div' to a categorical type, a numeric representation of the division\n",
    "df['Div'] = df['Div'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'teams' is a list of team names\n",
    "teams = pd.concat([df['HomeTeam'], df['AwayTeam']]).dropna().unique()\n",
    "teams.sort()\n",
    "\n",
    "# Creating a dictionary from team names to an incremental index number\n",
    "teams_dict = {team: index for index, team in enumerate(teams)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to a file\n",
    "with open(f'data/teams_dict_{content}.txt', 'w') as file:\n",
    "    file.write(str(teams_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique list of HomeTeam and AwayTeam names combined, and add an index to each team\n",
    "teams = pd.concat([df['HomeTeam'], df['AwayTeam']]).dropna().unique()\n",
    "\n",
    "# Sort the teams alphabetically\n",
    "teams.sort()\n",
    "\n",
    "# Convert to an array of dictionaries\n",
    "teams = [{'team': team, 'index': index} for index, team in enumerate(teams)]\n",
    "\n",
    "df['Team_ID'] = df['HomeTeam'].map(teams_dict)\n",
    "df['Opp_ID'] = df['AwayTeam'].map(teams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the column 'Referee' exists, convert it to a categorical type\n",
    "if 'Referee' in df.columns:\n",
    "    # Create a unique list of Referees, and add an index to each Referee\n",
    "    referees = pd.concat([df['Referee']]).unique()\n",
    "\n",
    "    # Convert to an array of dictionaries\n",
    "    referees = [{'referee': referee, 'index': index} for index, referee in enumerate(referees)]\n",
    "\n",
    "else:\n",
    "    df['Referee'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the creation of a unique list of Referees and adding an index to each Referee\n",
    "referees = df['Referee'].unique()  # This should directly refer to the 'Referee' column\n",
    "\n",
    "if len(referees) > 0:\n",
    "    # Convert to a dictionary with referee names as keys and their indices as values\n",
    "    referee_dict = {referee: index for index, referee in enumerate(referees)}\n",
    "\n",
    "    # Now map the 'Referee' column to these indices\n",
    "    df['Ref_ID'] = df['Referee'].map(referee_dict)\n",
    "else:\n",
    "    # If there are no referees, create a dummy column with all zeros\n",
    "    df['Ref_ID'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_date_to_int(date_str):\n",
    "    # Split the date_str by the \"/\" character into day, month, year\n",
    "    components = date_str.split('/')\n",
    "    \n",
    "    # If split was successful but not in expected format, try splitting by absence of separator for '%d%m%Y' or '%d%m%y'\n",
    "    if len(components) == 1:\n",
    "        if len(date_str) in [6, 8]:  # Length 6 for '%d%m%y', 8 for '%d%m%Y'\n",
    "            day, month = int(date_str[:2]), int(date_str[2:4])\n",
    "            year = int(date_str[4:])\n",
    "        else:\n",
    "            return 19000101  # Return default if format does not match expected\n",
    "    else:\n",
    "        day, month = int(components[0]), int(components[1])\n",
    "        year = int(components[2])\n",
    "    \n",
    "    # Adjust the year if it was only 2 characters long\n",
    "    if year < 100:\n",
    "        year += 2000\n",
    "    \n",
    "    # Create a date variable by using the day, month, year integers\n",
    "    # Note: Direct creation of date variable skipped to avoid unnecessary complexity,\n",
    "    # directly formatting to YYYYMMDD integer format instead.\n",
    "    date_int = int(f\"{year:04d}{month:02d}{day:02d}\")\n",
    "    \n",
    "    return date_int\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'Date' is the column you want to convert\n",
    "# First, ensure the Date column is in a datetime format if it's not already\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Apply the modified function\n",
    "df['Date_temp'] = df['Date'].apply(lambda x: parse_date_to_int(x.strftime('%d/%m/%Y')) if pd.notnull(x) else 19000101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the week as an integer\n",
    "df['DayOTW'] = df['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = df['Time'].fillna('00:00').str.replace(':', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the first 2 digits of the Time column, no decimals\n",
    "df['Time'] = df['Time'] // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [re.sub(r'[<]', '_st_', str(col)) for col in df.columns]\n",
    "df.columns = [re.sub(r'[>]', '_gt_', str(col)) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort df by Date_temp and Time\n",
    "df = df.sort_values(['Date_temp', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_vs_opponent_weighted(df, row, team_column):\n",
    "    # Initialize the total weighted score\n",
    "    weighted_score = 0\n",
    "    opponent_column = 'Opp_ID'\n",
    "\n",
    "    row_date_temp = row['Date'].year * 10000 + row['Date'].month * 100 + row['Date'].day\n",
    "\n",
    "    \n",
    "    # Filter the DataFrame for matches between the specified team and opponent from the same season, excluding the current match\n",
    "    filtered_matches = df[(df[team_column] == row[team_column]) & \n",
    "                          (df[opponent_column] == row[opponent_column]) &\n",
    "                          (df['Date_temp'] < row_date_temp)]\n",
    "    \n",
    "    recent_matches = filtered_matches.sort_values(by='Date', ascending=False).head(5)\n",
    "    \n",
    "    # Calculate weights - newer matches have higher weights\n",
    "    weights = range(len(recent_matches), 0, -1)  # Descending list based on the number of matches\n",
    "    \n",
    "    # Calculate score based on the match result\n",
    "    for match, weight in zip(recent_matches.itertuples(), weights):\n",
    "        if getattr(match, 'FTR') == 'H' and getattr(match, team_column) == getattr(match, 'Team_ID') or \\\n",
    "           getattr(match, 'FTR') == 'A' and getattr(match, team_column) != getattr(match, 'Team_ID'):\n",
    "            weighted_score += 3 * weight  # Team won\n",
    "        elif getattr(match, 'FTR') == 'A':\n",
    "            weighted_score += 1 * weight  # Draw\n",
    "        \n",
    "    # Normalize the weighted score by the sum of weights\n",
    "    normalized_weighted_score = weighted_score / sum(weights) if weights else 0\n",
    "\n",
    "    #print(f\"Weighted score: {weighted_score}, Normalized weighted score: {normalized_weighted_score}\")\n",
    "\n",
    "    return normalized_weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['team_hist_vs'] = [history_vs_opponent_weighted(df, row, 'Team_ID') for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modified function to create new columns\n",
    "df['team_hist_vs'] = df.apply(lambda x: history_vs_opponent_weighted(df, x, 'Team_ID'), axis=1)\n",
    "#df['opp_hist_vs'] = df.apply(lambda x: history_vs_opponent_weighted(df, x, 'Opp_ID'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function adapted for DataFrame application\n",
    "def convert_odds(row):\n",
    "    odds_win, odds_draw, odds_lose = row['AvgH'], row['AvgD'], row['AvgA']\n",
    "    prob_win = 1 / odds_win\n",
    "    prob_draw = 1 / odds_draw\n",
    "    prob_lose = 1 / odds_lose\n",
    "    prob_not_win = prob_draw + prob_lose\n",
    "    return pd.Series([prob_win, prob_not_win], index=['probs_win', 'probs_not_win'])\n",
    "\n",
    "# Apply the function and create new columns\n",
    "#df[['probs_win', 'probs_not_win']] = df.apply(convert_odds, axis=1)\n",
    "\n",
    "#df = df.drop(columns=['AvgH', 'AvgD', 'AvgA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_form(df, row, perspective):\n",
    "    # Determine the team ID based on the perspective ('Team' or 'Opp')\n",
    "    if perspective == 'Team':\n",
    "        team_id = row['Team_ID']\n",
    "    elif perspective == 'Opp':\n",
    "        team_id = row['Opp_ID']\n",
    "    else:\n",
    "        raise ValueError(\"Perspective must be 'Team' or 'Opp'\")\n",
    "    \n",
    "    # Get the current match date\n",
    "    current_date = row['Date_temp']\n",
    "    \n",
    "    # Filter past matches for the team\n",
    "    past_matches = df[((df['Team_ID'] == team_id) | (df['Opp_ID'] == team_id)) &\n",
    "                      (df['Date_temp'] < current_date)].sort_values(by='Date_temp', ascending=False).head(5)\n",
    "    \n",
    "    # Initialize points\n",
    "    points = 0\n",
    "    \n",
    "    # Weights for the matches (most recent match has the highest weight)\n",
    "    weights = [5, 4, 3, 2, 1]\n",
    "    \n",
    "    # Calculate points with weights\n",
    "    weighted_points_sum = 0\n",
    "    total_weights = sum(weights[:len(past_matches)])  # Adjust the total weight in case of less than 5 matches\n",
    "    \n",
    "    for match, weight in zip(past_matches.itertuples(), weights):\n",
    "        if (match.Team_ID == team_id and match.FTR == 'H') or (match.Opp_ID == team_id and match.FTR == 'A'):\n",
    "            points += 3\n",
    "        elif match.FTR == 'D':\n",
    "            points += 1\n",
    "        else:\n",
    "            points += 0\n",
    "\n",
    "        weighted_points_sum += points * weight\n",
    "    \n",
    "    if total_weights > 0:\n",
    "        return weighted_points_sum / total_weights\n",
    "    else:\n",
    "        return 0  # Return 0 if no past matches found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['team_form_team'] = [team_form(df, row, 'Team') for index, row in df.iterrows()]\n",
    "#df['team_form_opp'] = [team_form(df, row, 'Opp') for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to each row for 'Team'\n",
    "#df['team_form_team'] = df.apply(lambda row: team_form(df, row, 'Team'), axis=1)\n",
    "\n",
    "# Applying the function to each row for 'Opp'\n",
    "#df['team_form_opp'] = df.apply(lambda row: team_form(df, row, 'Opp'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avgs(df, row, perspective, home_column, away_column):\n",
    "    # Determine the team ID based on the perspective ('Team' or 'Opp')\n",
    "    if perspective == 'Team':\n",
    "        team_id = row['Team_ID']\n",
    "    elif perspective == 'Opp':\n",
    "        team_id = row['Opp_ID']\n",
    "    else:\n",
    "        raise ValueError(\"Perspective must be 'Team' or 'Opp'\")\n",
    "    \n",
    "    # Get the current match date\n",
    "    current_date = row['Date_temp']\n",
    "    \n",
    "    # Filter past 5 matches for the team\n",
    "    past_matches = df[((df['Team_ID'] == team_id) | (df['Opp_ID'] == team_id)) &\n",
    "                      (df['Date_temp'] < current_date)].sort_values(by='Date_temp', ascending=False).head(5)\n",
    "    \n",
    "    # Weights for the matches (most recent match has the highest weight)\n",
    "    weights = [5, 4, 3, 2, 1]\n",
    "    values = []\n",
    "    \n",
    "    # Determine which column to use and collect the values\n",
    "    for match in past_matches.itertuples():\n",
    "        if match.Team_ID == team_id:\n",
    "            values.append(getattr(match, home_column))  # Use home_column for home team\n",
    "        else:\n",
    "            values.append(getattr(match, away_column))  # Use away_column for away team\n",
    "    \n",
    "    # Calculate the weighted average of the values\n",
    "    weighted_sum = sum(value * weight for value, weight in zip(values, weights))\n",
    "    total_weights = sum(weights[:len(values)])  # Adjust total weight if there are less than 5 matches\n",
    "    \n",
    "    if total_weights > 0:\n",
    "        return weighted_sum / total_weights\n",
    "    else:\n",
    "        return 0  # Return 0 if no past matches found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['team_shots'] = df.apply(lambda row: rolling_avgs(df, row, 'Team', 'HS', 'AS'), axis=1)\n",
    "#df['opp_shots'] = df.apply(lambda row: rolling_avgs(df, row, 'Opp', 'HS', 'AS'), axis=1)\n",
    "\n",
    "#df['team_shots_target'] = df.apply(lambda row: rolling_avgs(df, row, 'Team', 'HST', 'AST'), axis=1)\n",
    "#df['opp_shots_target'] = df.apply(lambda row: rolling_avgs(df, row, 'Opp', 'HST', 'AST'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def avg_games_played(df, row, team_column):\n",
    "    team = row[team_column]\n",
    "    # Ensure current_match_date is a Timestamp for comparison\n",
    "    current_match_date = pd.to_datetime(row['Date'], dayfirst=True)  # Assuming 'Date' format is 'dd/mm/yy'\n",
    "\n",
    "    delta = 50\n",
    "    start_date = current_match_date - timedelta(days=delta)\n",
    "\n",
    "    # Ensure 'Date' column is in datetime format for comparison\n",
    "    #df['Date_temp'] = pd.to_datetime(df['Date'], dayfirst=True)  # Convert 'Date' column to datetime if not already done\n",
    "\n",
    "    # Filter the DataFrame for matches within the last 30 days\n",
    "    if team_column == 'Team_ID':\n",
    "        past_matches = df[((df[team_column] == team) | (df['Opp_ID'] == team)) &\n",
    "                          (df['Date'] >= start_date) & (df['Date'] < current_match_date)]\n",
    "    else:\n",
    "        past_matches = df[((df['Team_ID'] == team) | (df[team_column] == team)) &\n",
    "                          (df['Date'] >= start_date) & (df['Date'] < current_match_date)]\n",
    "\n",
    "    # If no matches were played in the last 30 days\n",
    "    if past_matches.empty:\n",
    "        return 0\n",
    "\n",
    "    # Calculate weights based on the recency of each match\n",
    "    weights = (current_match_date - past_matches['Date']).dt.days\n",
    "    weighted_count = sum(delta - weights + 1)  # '+ 1' to include the match day in the weight\n",
    "\n",
    "    # Normalize weights to sum to 1 and calculate the weighted average\n",
    "    total_weight = sum(delta - weights + 1)\n",
    "    weighted_avg = weighted_count / total_weight\n",
    "\n",
    "    return weighted_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function for each team and opponent\n",
    "#df['team_avg_games'] = df.apply(lambda x: avg_games_played(df, x, 'Team_ID'), axis=1)\n",
    "#df['opp_avg_games'] = df.apply(lambda x: avg_games_played(df, x, 'Opp_ID'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means only for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "means = df[numeric_cols].mean()\n",
    "\n",
    "# Fill missing values in numeric columns with their respective means\n",
    "df[numeric_cols] = df[numeric_cols].fillna(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the FTR to 'X' where the value is currently NaN\n",
    "df['FTR'] = df['FTR'].fillna('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every row where 'FTR' is not 'H', 'D', or 'A', or 'X' (if future matches are included)\n",
    "df = df[df['FTR'].isin(['H', 'D', 'A', 'X'])]\n",
    "\n",
    "# FTR2 to store the FTR as '0', '1', or '2' for future reference\n",
    "df['FTR2'] = df['FTR'].map({'H': 1, 'D': 0, 'A': 2, 'X': -1}).astype(int)\n",
    "\n",
    "# Map 'H', 'D', and 'A' to 1, 0, and 0 respectively\n",
    "df['FTR'] = df['FTR'].map({'H': 1, 'D': 0, 'A': 2, 'X': -1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "    \n",
    "        'Div', 'Date_temp', 'Time', 'DayOTW', 'Team_ID', 'Opp_ID', 'Ref_ID', 'FTR', 'FTR2',\n",
    "        'team_hist_vs', \n",
    "        #'opp_hist_vs',\n",
    "\n",
    "        #'probs_win',         \n",
    "        #'probs_not_win', \n",
    "        \n",
    "        #'team_form_team', \n",
    "        #'team_form_opp',\n",
    "         \n",
    "        #'team_shots', 'opp_shots',\n",
    "        #'team_shots_target', 'opp_shots_target',\n",
    "\n",
    "        #'team_avg_games', 'opp_avg_games',\n",
    "\n",
    "        'AvgH', 'AvgD', 'AvgA'\n",
    "         \n",
    "         \n",
    "         ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Date_temp' to 'Date'\n",
    "df.rename(columns={'Date_temp': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df to a CSV file\n",
    "df.to_csv(f'data/processed/processed_data_{content}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 400  # Set Frequency To 2500 Hertz\n",
    "duration = 200  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

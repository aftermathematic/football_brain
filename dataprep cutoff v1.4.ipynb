{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler  # Assuming you might need it\n",
    "\n",
    "# Specific models and tools\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Encoding and feature selection\n",
    "from category_encoders import TargetEncoder  # Fixed the import based on usage\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Miscellaneous settings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = [\n",
    "    'E0', \n",
    "    'E1', \n",
    "    \n",
    "    'E2', 'E3',  \n",
    "    'SC0', 'SC1',    \n",
    "    \n",
    "    'D1',     'D2',    \n",
    "    'F1',     'F2',    \n",
    "    'I1',     'I2',    \n",
    "    'SP1',    'SP2',    \n",
    "    \n",
    "    'B1',    'G1',    'N1',    'P1',    'T1',\n",
    "]\n",
    "\n",
    "seasons = [\n",
    "    '2324', \n",
    "    '2223', \n",
    "    '2122', \n",
    "    '2021',\n",
    "    '1920',\n",
    "    '1819',\n",
    "    '1718',\n",
    "    '1617',\n",
    "    '1516',\n",
    "    '1415'\n",
    "]\n",
    "\n",
    "fixtures = [\n",
    "    'fixtures',\n",
    "    #'new_league_fixtures'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataprep_start_date to the date the data preparation should start\n",
    "# If None, the data preparation will start from the beginning of the data\n",
    "\n",
    "# Make sure the file below already exists if you want to start from a specific date\n",
    "# file should be in the format \"processed_data_<content>.csv\"\n",
    "content = \"allcomps_10s\"\n",
    "\n",
    "#dataprep_start_date = None\n",
    "dataprep_start_date = pd.Timestamp(year=2024, month=4, day=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_files = []\n",
    "fixtures_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in seasons:    \n",
    "    for comp in comps:  \n",
    "        matches_files.append('data/scraped/%s/%s.csv' % (season, comp))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fixture in fixtures:    \n",
    "    fixtures_files.append(f'data/scraped/{seasons[0]}/{fixture}.csv')\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            year = re.search(r'(\\d{4})', file).group(1)\n",
    "            print(f'Loading {file}')\n",
    "\n",
    "            # Try to read with default utf-8 encoding\n",
    "            try:\n",
    "                df_temp = pd.read_csv(file, encoding='utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                # If utf-8 decoding fails, try reading with ISO-8859-1\n",
    "                df_temp = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "\n",
    "            df_temp['Season'] = year\n",
    "            df = pd.concat([df, df_temp], ignore_index=True)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Error: {file} not found')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading {file}: {e}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/scraped/2324/E0.csv\n",
      "Loading data/scraped/2324/E1.csv\n",
      "Loading data/scraped/2324/E2.csv\n",
      "Loading data/scraped/2324/E3.csv\n",
      "Loading data/scraped/2324/SC0.csv\n",
      "Loading data/scraped/2324/SC1.csv\n",
      "Loading data/scraped/2324/D1.csv\n",
      "Loading data/scraped/2324/D2.csv\n",
      "Loading data/scraped/2324/F1.csv\n",
      "Loading data/scraped/2324/F2.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/scraped/2324/I1.csv\n",
      "Loading data/scraped/2324/I2.csv\n",
      "Loading data/scraped/2324/SP1.csv\n",
      "Loading data/scraped/2324/SP2.csv\n",
      "Loading data/scraped/2324/B1.csv\n",
      "Loading data/scraped/2324/G1.csv\n",
      "Loading data/scraped/2324/N1.csv\n",
      "Loading data/scraped/2324/P1.csv\n",
      "Loading data/scraped/2324/T1.csv\n",
      "Loading data/scraped/2223/E0.csv\n",
      "Loading data/scraped/2223/E1.csv\n",
      "Loading data/scraped/2223/E2.csv\n",
      "Loading data/scraped/2223/E3.csv\n",
      "Loading data/scraped/2223/SC0.csv\n",
      "Loading data/scraped/2223/SC1.csv\n",
      "Loading data/scraped/2223/D1.csv\n",
      "Loading data/scraped/2223/D2.csv\n",
      "Loading data/scraped/2223/F1.csv\n",
      "Loading data/scraped/2223/F2.csv\n",
      "Loading data/scraped/2223/I1.csv\n",
      "Loading data/scraped/2223/I2.csv\n",
      "Loading data/scraped/2223/SP1.csv\n",
      "Loading data/scraped/2223/SP2.csv\n",
      "Loading data/scraped/2223/B1.csv\n",
      "Loading data/scraped/2223/G1.csv\n",
      "Loading data/scraped/2223/N1.csv\n",
      "Loading data/scraped/2223/P1.csv\n",
      "Loading data/scraped/2223/T1.csv\n",
      "Loading data/scraped/2122/E0.csv\n",
      "Loading data/scraped/2122/E1.csv\n",
      "Loading data/scraped/2122/E2.csv\n",
      "Loading data/scraped/2122/E3.csv\n",
      "Loading data/scraped/2122/SC0.csv\n",
      "Loading data/scraped/2122/SC1.csv\n",
      "Loading data/scraped/2122/D1.csv\n",
      "Loading data/scraped/2122/D2.csv\n",
      "Loading data/scraped/2122/F1.csv\n",
      "Loading data/scraped/2122/F2.csv\n",
      "Loading data/scraped/2122/I1.csv\n",
      "Loading data/scraped/2122/I2.csv\n",
      "Loading data/scraped/2122/SP1.csv\n",
      "Loading data/scraped/2122/SP2.csv\n",
      "Loading data/scraped/2122/B1.csv\n",
      "Loading data/scraped/2122/G1.csv\n",
      "Loading data/scraped/2122/N1.csv\n",
      "Loading data/scraped/2122/P1.csv\n",
      "Loading data/scraped/2122/T1.csv\n",
      "Loading data/scraped/2021/E0.csv\n",
      "Loading data/scraped/2021/E1.csv\n",
      "Loading data/scraped/2021/E2.csv\n",
      "Loading data/scraped/2021/E3.csv\n",
      "Loading data/scraped/2021/SC0.csv\n",
      "Loading data/scraped/2021/SC1.csv\n",
      "Loading data/scraped/2021/D1.csv\n",
      "Loading data/scraped/2021/D2.csv\n",
      "Loading data/scraped/2021/F1.csv\n",
      "Loading data/scraped/2021/F2.csv\n",
      "Loading data/scraped/2021/I1.csv\n",
      "Loading data/scraped/2021/I2.csv\n",
      "Loading data/scraped/2021/SP1.csv\n",
      "Loading data/scraped/2021/SP2.csv\n",
      "Loading data/scraped/2021/B1.csv\n",
      "Loading data/scraped/2021/G1.csv\n",
      "Loading data/scraped/2021/N1.csv\n",
      "Loading data/scraped/2021/P1.csv\n",
      "Loading data/scraped/2021/T1.csv\n",
      "Loading data/scraped/1920/E0.csv\n",
      "Loading data/scraped/1920/E1.csv\n",
      "Loading data/scraped/1920/E2.csv\n",
      "Loading data/scraped/1920/E3.csv\n",
      "Loading data/scraped/1920/SC0.csv\n",
      "Loading data/scraped/1920/SC1.csv\n",
      "Loading data/scraped/1920/D1.csv\n",
      "Loading data/scraped/1920/D2.csv\n",
      "Loading data/scraped/1920/F1.csv\n",
      "Loading data/scraped/1920/F2.csv\n",
      "Loading data/scraped/1920/I1.csv\n",
      "Loading data/scraped/1920/I2.csv\n",
      "Loading data/scraped/1920/SP1.csv\n",
      "Loading data/scraped/1920/SP2.csv\n",
      "Loading data/scraped/1920/B1.csv\n",
      "Loading data/scraped/1920/G1.csv\n",
      "Loading data/scraped/1920/N1.csv\n",
      "Loading data/scraped/1920/P1.csv\n",
      "Loading data/scraped/1920/T1.csv\n",
      "Loading data/scraped/1819/E0.csv\n",
      "Loading data/scraped/1819/E1.csv\n",
      "Loading data/scraped/1819/E2.csv\n",
      "Loading data/scraped/1819/E3.csv\n",
      "Loading data/scraped/1819/SC0.csv\n",
      "Loading data/scraped/1819/SC1.csv\n",
      "Loading data/scraped/1819/D1.csv\n",
      "Loading data/scraped/1819/D2.csv\n",
      "Loading data/scraped/1819/F1.csv\n",
      "Loading data/scraped/1819/F2.csv\n",
      "Loading data/scraped/1819/I1.csv\n",
      "Loading data/scraped/1819/I2.csv\n",
      "Loading data/scraped/1819/SP1.csv\n",
      "Loading data/scraped/1819/SP2.csv\n",
      "Loading data/scraped/1819/B1.csv\n",
      "Loading data/scraped/1819/G1.csv\n",
      "Loading data/scraped/1819/N1.csv\n",
      "Loading data/scraped/1819/P1.csv\n",
      "Loading data/scraped/1819/T1.csv\n",
      "Loading data/scraped/1718/E0.csv\n",
      "Loading data/scraped/1718/E1.csv\n",
      "Loading data/scraped/1718/E2.csv\n",
      "Loading data/scraped/1718/E3.csv\n",
      "Loading data/scraped/1718/SC0.csv\n",
      "Loading data/scraped/1718/SC1.csv\n",
      "Loading data/scraped/1718/D1.csv\n",
      "Loading data/scraped/1718/D2.csv\n",
      "Loading data/scraped/1718/F1.csv\n",
      "Loading data/scraped/1718/F2.csv\n",
      "Loading data/scraped/1718/I1.csv\n",
      "Loading data/scraped/1718/I2.csv\n",
      "Loading data/scraped/1718/SP1.csv\n",
      "Loading data/scraped/1718/SP2.csv\n",
      "Loading data/scraped/1718/B1.csv\n",
      "Loading data/scraped/1718/G1.csv\n",
      "Loading data/scraped/1718/N1.csv\n",
      "Loading data/scraped/1718/P1.csv\n",
      "Loading data/scraped/1718/T1.csv\n",
      "Loading data/scraped/1617/E0.csv\n",
      "Loading data/scraped/1617/E1.csv\n",
      "Loading data/scraped/1617/E2.csv\n",
      "Loading data/scraped/1617/E3.csv\n",
      "Loading data/scraped/1617/SC0.csv\n",
      "Loading data/scraped/1617/SC1.csv\n",
      "Loading data/scraped/1617/D1.csv\n",
      "Loading data/scraped/1617/D2.csv\n",
      "Loading data/scraped/1617/F1.csv\n",
      "Loading data/scraped/1617/F2.csv\n",
      "Loading data/scraped/1617/I1.csv\n",
      "Loading data/scraped/1617/I2.csv\n",
      "Loading data/scraped/1617/SP1.csv\n",
      "Loading data/scraped/1617/SP2.csv\n",
      "Loading data/scraped/1617/B1.csv\n",
      "Loading data/scraped/1617/G1.csv\n",
      "Loading data/scraped/1617/N1.csv\n",
      "Loading data/scraped/1617/P1.csv\n",
      "Loading data/scraped/1617/T1.csv\n",
      "Loading data/scraped/1516/E0.csv\n",
      "Loading data/scraped/1516/E1.csv\n",
      "Loading data/scraped/1516/E2.csv\n",
      "Loading data/scraped/1516/E3.csv\n",
      "Loading data/scraped/1516/SC0.csv\n",
      "Loading data/scraped/1516/SC1.csv\n",
      "Loading data/scraped/1516/D1.csv\n",
      "Loading data/scraped/1516/D2.csv\n",
      "Loading data/scraped/1516/F1.csv\n",
      "Loading data/scraped/1516/F2.csv\n",
      "Loading data/scraped/1516/I1.csv\n",
      "Loading data/scraped/1516/I2.csv\n",
      "Loading data/scraped/1516/SP1.csv\n",
      "Loading data/scraped/1516/SP2.csv\n",
      "Loading data/scraped/1516/B1.csv\n",
      "Loading data/scraped/1516/G1.csv\n",
      "Loading data/scraped/1516/N1.csv\n",
      "Loading data/scraped/1516/P1.csv\n",
      "Loading data/scraped/1516/T1.csv\n",
      "Loading data/scraped/1415/E0.csv\n",
      "Loading data/scraped/1415/E1.csv\n",
      "Loading data/scraped/1415/E2.csv\n",
      "Loading data/scraped/1415/E3.csv\n",
      "Loading data/scraped/1415/SC0.csv\n",
      "Loading data/scraped/1415/SC1.csv\n",
      "Loading data/scraped/1415/D1.csv\n",
      "Loading data/scraped/1415/D2.csv\n",
      "Loading data/scraped/1415/F1.csv\n",
      "Loading data/scraped/1415/F2.csv\n",
      "Loading data/scraped/1415/I1.csv\n",
      "Loading data/scraped/1415/I2.csv\n",
      "Loading data/scraped/1415/SP1.csv\n",
      "Loading data/scraped/1415/SP2.csv\n",
      "Loading data/scraped/1415/B1.csv\n",
      "Loading data/scraped/1415/G1.csv\n",
      "Loading data/scraped/1415/N1.csv\n",
      "Loading data/scraped/1415/P1.csv\n",
      "Loading data/scraped/1415/T1.csv\n",
      "Loading data/scraped/2324/fixtures.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data into DataFrames\n",
    "df = load_data(matches_files)\n",
    "df_fixtures = load_data(fixtures_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67904, 184)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df_fixtures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_to_int(date_str):\n",
    "    # Split the date_str by the \"/\" character into day, month, year\n",
    "    components = date_str.split('/')\n",
    "    \n",
    "    # If split was successful but not in expected format, try splitting by absence of separator for '%d%m%Y' or '%d%m%y'\n",
    "    if len(components) == 1:\n",
    "        if len(date_str) in [6, 8]:  # Length 6 for '%d%m%y', 8 for '%d%m%Y'\n",
    "            day, month = int(date_str[:2]), int(date_str[2:4])\n",
    "            year = int(date_str[4:])\n",
    "        else:\n",
    "            return 19000101  # Return default if format does not match expected\n",
    "    else:\n",
    "        day, month = int(components[0]), int(components[1])\n",
    "        year = int(components[2])\n",
    "    \n",
    "    # Adjust the year if it was only 2 characters long\n",
    "    if year < 100:\n",
    "        year += 2000\n",
    "    \n",
    "    # Create a date variable by using the day, month, year integers\n",
    "    # Note: Direct creation of date variable skipped to avoid unnecessary complexity,\n",
    "    # directly formatting to YYYYMMDD integer format instead.\n",
    "    date_int = int(f\"{year:04d}{month:02d}{day:02d}\")\n",
    "    \n",
    "    return date_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_fixtures) > 0:\n",
    "\n",
    "    # Parse the 'Date' column to a datetime object\n",
    "    df_fixtures['Date_temp'] = pd.to_datetime(df_fixtures['Date'], format='%d/%m/%Y')\n",
    "\n",
    "    # Convert the datetime object to an integer in the format YYYYMMDD\n",
    "    df_fixtures['Date_temp'] = df_fixtures['Date_temp'].apply(\n",
    "        lambda x: int(x.strftime('%Y%m%d')) if pd.notnull(x) else 19000101)\n",
    "\n",
    "    # Replace all values with -1 in FTR column\n",
    "    df_fixtures['FTR'].fillna('X', inplace=True)\n",
    "\n",
    "    # Find the lowest fixture date\n",
    "    # This is the date where the data preparation will start\n",
    "    fixture_cutoff = df_fixtures['Date'].min()\n",
    "\n",
    "    # Remove all the rows in df that are after the fixture_cutoff date\n",
    "    df = df[df['Date'] < fixture_cutoff]\n",
    "\n",
    "    # Concatenate the matches and fixtures dataframes\n",
    "    df = pd.concat([df, df_fixtures], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate column names\n",
    "print(df.columns[df.columns.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the rows in the dataframe where the 'Div' is not in the list of comps\n",
    "df = df[df['Div'].isin(comps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max index: 18\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for all competitions\n",
    "\n",
    "file_path = f\"data/comps_dict_{content}.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the dictionary from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        comps_dict = eval(file.read())  # Using eval to convert string back to dictionary\n",
    "    # Find the maximum index currently in the dictionary\n",
    "    max_index = max(comps_dict.values())\n",
    "\n",
    "    print(f\"max index: {max_index}\")\n",
    "else:\n",
    "    comps_dict = {}\n",
    "    max_index = -1  \n",
    "\n",
    "# Get all unique divisions from DataFrame\n",
    "all_comps = df['Div'].dropna().unique()\n",
    "all_comps.sort()\n",
    "\n",
    "# Create a dictionary of new divisions alone\n",
    "new_comps = {div: index for index, div in enumerate(all_comps, start=max_index + 1) if div not in comps_dict}\n",
    "\n",
    "# Update dictionary only with new divisions\n",
    "comps_dict.update(new_comps)\n",
    "\n",
    "# Save the updated dictionary to a file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(str(comps_dict))\n",
    "\n",
    "# Add division ID column to DataFrame\n",
    "df['Div'] = df['Div'].map(comps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max index: 516\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for all teams\n",
    "\n",
    "file_path = f\"data/teams_dict_{content}.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the dictionary from the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        teams_dict = eval(file.read())  \n",
    "    max_index = max(teams_dict.values())\n",
    "\n",
    "    print(f\"max index: {max_index}\")\n",
    "else:\n",
    "    teams_dict = {}\n",
    "    max_index = -1 \n",
    "\n",
    "# Get all teams from DataFrame\n",
    "all_teams = pd.concat([df['HomeTeam'], df['AwayTeam']]).dropna().unique()\n",
    "all_teams.sort()\n",
    "\n",
    "# Create a dictionary of new teams alone\n",
    "new_teams = {team: index for index, team in enumerate(all_teams) if team not in teams_dict}\n",
    "\n",
    "# Update dictionary only with new teams, starting indices from max_index + 1\n",
    "start_index = max_index + 1\n",
    "teams_dict.update({team: index + start_index for index, team in enumerate(new_teams) if team not in teams_dict})\n",
    "\n",
    "# Save the updated dictionary to a file\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(str(teams_dict))\n",
    "\n",
    "# Add team ID columns to DataFrame\n",
    "df['Team_ID'] = df['HomeTeam'].map(teams_dict)\n",
    "df['Opp_ID'] = df['AwayTeam'].map(teams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_duplicates(df):\n",
    "    # Sort the DataFrame so that rows with 'FTR' == -1 come first\n",
    "    df.sort_values(by=['Date', 'Team_ID', 'Opp_ID', 'FTR'], ascending=[True, True, True, False], inplace=True)\n",
    "    \n",
    "    # Drop duplicates based on 'Date', 'Team_ID', and 'Opp_ID' keeping the first occurrence (where 'FTR' is -1)\n",
    "    df = df.drop_duplicates(subset=['Date', 'Team_ID', 'Opp_ID'], keep='first')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = clean_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ELO ratings for each team\n",
    "\n",
    "# Initialize ratings dictionary\n",
    "teams = pd.concat([df['Team_ID'], df['Opp_ID']]).unique()\n",
    "ratings = {team: 1500 for team in teams}\n",
    "\n",
    "def calculate_expected_score(rating_a, rating_b):\n",
    "    return 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
    "\n",
    "def update_elo(rating, actual_score, expected_score, k=30):\n",
    "\n",
    "    rating = rating + k * (actual_score - expected_score)\n",
    "\n",
    "    # Parse the rating as an integer with no decimal points\n",
    "    return int(rating)\n",
    "\n",
    "# Iterate over the DataFrame and update ELO ratings after each match\n",
    "elo_team = []\n",
    "elo_opp = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    home_team, away_team, home_score, away_score = row['Team_ID'], row['Opp_ID'], row['FTHG'], row['FTAG']\n",
    "    home_rating = ratings[home_team]\n",
    "    away_rating = ratings[away_team]\n",
    "    \n",
    "    # Calculate expected scores\n",
    "    expected_home = calculate_expected_score(home_rating, away_rating)\n",
    "    expected_away = calculate_expected_score(away_rating, home_rating)\n",
    "    \n",
    "    # Calculate actual scores\n",
    "    actual_home = 1 if home_score > away_score else 0.5 if home_score == away_score else 0\n",
    "    actual_away = 1 - actual_home\n",
    "    \n",
    "    # Update ratings\n",
    "    new_home_rating = update_elo(home_rating, actual_home, expected_home)\n",
    "    new_away_rating = update_elo(away_rating, actual_away, expected_away)\n",
    "    \n",
    "    # Store updated ratings in the ratings dictionary\n",
    "    ratings[home_team] = new_home_rating\n",
    "    ratings[away_team] = new_away_rating\n",
    "    \n",
    "    # Append current ratings to list\n",
    "    elo_team.append(new_home_rating)\n",
    "    elo_opp.append(new_away_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new ELO ratings to the DataFrame\n",
    "df['team_elo'] = elo_team\n",
    "df['opp_elo'] = elo_opp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Apply the modified function\n",
    "df['Date_temp'] = df['Date'].apply(lambda x: parse_date_to_int(x.strftime('%d/%m/%Y')) if pd.notnull(x) else 19000101)\n",
    "\n",
    "# Day of the week as an integer\n",
    "df['DayOTW'] = df['Date'].dt.dayofweek\n",
    "\n",
    "df['Time'] = df['Time'].fillna('00:00').str.replace(':', '').astype(int)\n",
    "\n",
    "# Only keep the first 2 digits of the Time column, no decimals\n",
    "df['Time'] = df['Time'] // 100\n",
    "\n",
    "# Sort df by Date_temp and Time\n",
    "df = df.sort_values(['Date_temp', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [re.sub(r'[<]', '_st_', str(col)) for col in df.columns]\n",
    "df.columns = [re.sub(r'[>]', '_gt_', str(col)) for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points(df, row, team_column):\n",
    "    # Season of the current match\n",
    "    current_season = row['Season']\n",
    "    \n",
    "    # Date of the current match\n",
    "    current_date = row['Date']\n",
    "\n",
    "    # Define the opponent column based on the team column\n",
    "    opponent_column = 'Opp_ID' if team_column == 'Team_ID' else 'Team_ID'\n",
    "\n",
    "    # Filter DataFrame for matches from the same season before the current date\n",
    "    past_matches = df[\n",
    "        (df['Season'] == current_season) & \n",
    "        (df['Date'] < current_date) &\n",
    "        ((df['Team_ID'] == row[team_column]) | (df['Opp_ID'] == row[team_column]))\n",
    "    ]\n",
    "\n",
    "    # Initialize total points\n",
    "    total_points = 0\n",
    "\n",
    "    # Calculate points for each past match\n",
    "    for match in past_matches.itertuples():\n",
    "        if getattr(match, 'Team_ID') == row[team_column]:\n",
    "            if getattr(match, 'FTR') == 'H':\n",
    "                total_points += 3  # Home win\n",
    "            elif getattr(match, 'FTR') == 'D':\n",
    "                total_points += 1  # Draw\n",
    "        elif getattr(match, 'Opp_ID') == row[team_column]:\n",
    "            if getattr(match, 'FTR') == 'A':\n",
    "                total_points += 3  # Away win\n",
    "            elif getattr(match, 'FTR') == 'D':\n",
    "                total_points += 1  # Draw\n",
    "\n",
    "    # Calculate average points if there are any matches played\n",
    "    matches_played = len(past_matches)\n",
    "    if matches_played > 0:\n",
    "        avg_points = total_points / matches_played\n",
    "    else:\n",
    "        avg_points = 0\n",
    "\n",
    "    # Round to 3 decimal places\n",
    "    return round(avg_points, 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team_points'] = df.apply(lambda x: points(df, x, 'Team_ID') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)\n",
    "df['opp_points'] = df.apply(lambda x: points(df, x, 'Opp_ID') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_vs_opponent_weighted(df, row, team_column):\n",
    "    # Determine opponent column based on team column\n",
    "    opponent_column = 'Team_ID' if team_column == 'Opp_ID' else 'Opp_ID'\n",
    "\n",
    "    # Combine year, month, and day into an integer 'Date_temp'\n",
    "    row_date_temp = row['Date'].year * 10000 + row['Date'].month * 100 + row['Date'].day\n",
    "\n",
    "    # Filter for matches between specified teams, excluding current match\n",
    "    mask = (\n",
    "        ((df[team_column] == row[team_column]) & (df[opponent_column] == row[opponent_column])) |\n",
    "        ((df[team_column] == row[opponent_column]) & (df[opponent_column] == row[team_column]))\n",
    "    ) & (df['Date_temp'] < row_date_temp)\n",
    "\n",
    "    filtered_matches = df[mask]\n",
    "    \n",
    "    if filtered_matches.empty:\n",
    "        return 0  # Return early if no matches found\n",
    "\n",
    "    # Sort by date and select top 5 recent matches\n",
    "    recent_matches = filtered_matches.sort_values(by='Date', ascending=False).head(5)\n",
    "    weights = list(range(len(recent_matches), 0, -1))  # Descending weights\n",
    "\n",
    "    # Calculate weighted score based on match results\n",
    "    weighted_score = sum(\n",
    "        (3 * weight if match.FTR == 'H' and match.__getattribute__(team_column) == match.Team_ID or\n",
    "                      match.FTR == 'A' and match.__getattribute__(team_column) != match.Team_ID else\n",
    "         1 * weight if match.FTR == 'D' else 0)\n",
    "        for match, weight in zip(recent_matches.itertuples(), weights)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize the weighted score by the sum of weights\n",
    "    return round(weighted_score / sum(weights), 3) if weights else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team_hist_vs'] = df.apply(lambda x: history_vs_opponent_weighted(df, x, 'Team_ID') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)\n",
    "df['opp_hist_vs'] = df.apply(lambda x: history_vs_opponent_weighted(df, x, 'Opp_ID') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_form(df, row, perspective):\n",
    "    # Determine the team ID based on the perspective ('Team' or 'Opp')\n",
    "    if perspective == 'Team':\n",
    "        team_id = row['Team_ID']\n",
    "    elif perspective == 'Opp':\n",
    "        team_id = row['Opp_ID']\n",
    "    else:\n",
    "        raise ValueError(\"Perspective must be 'Team' or 'Opp'\")\n",
    "    \n",
    "    # Get the current match date\n",
    "    current_date = row['Date_temp']\n",
    "    \n",
    "    # Filter past matches for the team\n",
    "    past_matches = df[((df['Team_ID'] == team_id) | (df['Opp_ID'] == team_id)) &\n",
    "                      (df['Date_temp'] < current_date)].sort_values(by='Date_temp', ascending=False).head(5)\n",
    "    \n",
    "    # Initialize points\n",
    "    points = 0\n",
    "    \n",
    "    # Weights for the matches (most recent match has the highest weight)\n",
    "    weights = [5, 4, 3, 2, 1]\n",
    "    \n",
    "    # Calculate points with weights\n",
    "    weighted_points_sum = 0\n",
    "    total_weights = sum(weights[:len(past_matches)])  # Adjust the total weight in case of less than 5 matches\n",
    "    \n",
    "    for match, weight in zip(past_matches.itertuples(), weights):\n",
    "        if (match.Team_ID == team_id and match.FTR == 'H') or (match.Opp_ID == team_id and match.FTR == 'A'):\n",
    "            points += 3\n",
    "        elif match.FTR == 'D':\n",
    "            points += 1\n",
    "        else:\n",
    "            points += 0\n",
    "\n",
    "        weighted_points_sum += points * weight\n",
    "    \n",
    "    if total_weights > 0:\n",
    "\n",
    "        team_form = round(weighted_points_sum / total_weights, 2)\n",
    "\n",
    "        return team_form\n",
    "    else:\n",
    "        return 0  # Return 0 if no past matches found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team_form'] = df.apply(lambda x: team_form(df, x, 'Team') if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)\n",
    "df['opp_form'] = df.apply(lambda x: team_form(df, x, 'Opp') if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_avgs_combined(df, row, perspective):\n",
    "    # Determine the team ID based on the perspective ('Team' or 'Opp')\n",
    "    if perspective == 'Team':\n",
    "        team_id = row['Team_ID']\n",
    "    elif perspective == 'Opp':\n",
    "        team_id = row['Opp_ID']\n",
    "    else:\n",
    "        raise ValueError(\"Perspective must be 'Team' or 'Opp'\")\n",
    "    \n",
    "    # Get the current match date\n",
    "    current_date = row['Date_temp']\n",
    "    \n",
    "    # Filter past 5 matches for the team\n",
    "    past_matches = df[((df['Team_ID'] == team_id) | (df['Opp_ID'] == team_id)) &\n",
    "                      (df['Date_temp'] < current_date)].sort_values(by='Date_temp', ascending=False).head(5)\n",
    "    \n",
    "    # Weights for the matches (most recent match has the highest weight)\n",
    "    weights = [5, 4, 3, 2, 1]\n",
    "    \n",
    "    # Initialize sums and weighted sums\n",
    "    shots = []\n",
    "    shots_target = []\n",
    "    \n",
    "    # Determine which columns to use and collect the values\n",
    "    for match in past_matches.itertuples():\n",
    "        if match.Team_ID == team_id:\n",
    "            shots.append(getattr(match, 'HS'))  # Home shots\n",
    "            shots_target.append(getattr(match, 'HST'))  # Home shots on target\n",
    "        else:\n",
    "            shots.append(getattr(match, 'AS'))  # Away shots\n",
    "            shots_target.append(getattr(match, 'AST'))  # Away shots on target\n",
    "    \n",
    "    # Calculate the weighted averages of the values\n",
    "    weighted_shots = sum(s * w for s, w in zip(shots, weights))\n",
    "    weighted_shots_target = sum(st * w for st, w in zip(shots_target, weights))\n",
    "    total_weights = sum(weights[:len(shots)])  # Adjust total weight if there are less than 5 matches\n",
    "    \n",
    "    avg_shots = weighted_shots / total_weights if total_weights > 0 else 0\n",
    "    avg_shots_target = weighted_shots_target / total_weights if total_weights > 0 else 0\n",
    "\n",
    "    # Round the averages to 2 decimal places\n",
    "    avg_shots = round(avg_shots, 2)\n",
    "    avg_shots_target = round(avg_shots_target, 2)\n",
    "    \n",
    "    return avg_shots, avg_shots_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team_shots'], df['team_shots_target'] = zip(*df.apply(lambda x: rolling_avgs_combined(df, x, 'Team') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else (0, 0), axis=1))\n",
    "df['opp_shots'], df['opp_shots_target'] = zip(*df.apply(lambda x: rolling_avgs_combined(df, x, 'Opp') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else (0, 0), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_goals(df, row, team_column):\n",
    "    # Season and date of the current match\n",
    "    current_season = row['Season']\n",
    "    current_date = row['Date']\n",
    "\n",
    "    # Determine the columns for goals scored and conceded based on perspective\n",
    "    if team_column == 'Team_ID':\n",
    "        goals_scored_column = 'FTHG'  # Assuming FTHG is the column for home team goals\n",
    "        goals_conceded_column = 'FTAG'  # Assuming FTAG is the column for away team goals\n",
    "    else:\n",
    "        goals_scored_column = 'FTAG'  # Flip the columns if we are looking from the opponent's perspective\n",
    "        goals_conceded_column = 'FTHG'\n",
    "\n",
    "    # Filter matches from the same season and before the current date\n",
    "    past_matches = df[\n",
    "        (df['Season'] == current_season) & \n",
    "        (df['Date'] < current_date) & \n",
    "        ((df['Team_ID'] == row[team_column]) | (df['Opp_ID'] == row[team_column]))\n",
    "    ]\n",
    "\n",
    "    # Calculate the average goals scored and conceded\n",
    "    goals_scored = 0\n",
    "    goals_conceded = 0\n",
    "    total_matches = len(past_matches)\n",
    "\n",
    "    for match in past_matches.itertuples():\n",
    "        if getattr(match, 'Team_ID') == row[team_column]: \n",
    "            goals_scored += getattr(match, goals_scored_column)\n",
    "            goals_conceded += getattr(match, goals_conceded_column)\n",
    "        else:  # Team is playing away\n",
    "            goals_scored += getattr(match, goals_scored_column)\n",
    "            goals_conceded += getattr(match, goals_conceded_column)\n",
    "\n",
    "    avg_goals_for = goals_scored / total_matches if total_matches > 0 else 0\n",
    "    avg_goals_against = goals_conceded / total_matches if total_matches > 0 else 0\n",
    "\n",
    "    avg_goals_for = round(avg_goals_for, 2)\n",
    "    avg_goals_against = round(avg_goals_against, 2)\n",
    "\n",
    "    return avg_goals_for, avg_goals_against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function and create new columns\n",
    "df['team_avg_goals_for'], df['team_avg_goals_against'] = zip(*df.apply(lambda x: avg_goals(df, x, 'Team_ID') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else (0, 0), axis=1))\n",
    "df['opp_avg_goals_for'], df['opp_avg_goals_against'] = zip(*df.apply(lambda x: avg_goals(df, x, 'Opp_ID') \n",
    "    if dataprep_start_date is None or x['Date'] >= dataprep_start_date else (0, 0), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means only for numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "means = df[numeric_cols].mean()\n",
    "\n",
    "# Fill missing values in numeric columns with their respective means\n",
    "df[numeric_cols] = df[numeric_cols].fillna(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the FTR to 'X' where the value is currently NaN\n",
    "df['FTR'] = df['FTR'].fillna('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop every row where 'FTR' is not 'H', 'D', or 'A', or 'X' (if future matches are included)\n",
    "df = df[df['FTR'].isin(['H', 'D', 'A', 'X'])]\n",
    "\n",
    "# Map 'H', 'D', and 'A' to 0, 1, and 2 respectively\n",
    "df['FTR'] = df['FTR'].map({'H': 0, 'D': 1, 'A': 2, 'X': -1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_xg(df, row, team_column):\n",
    "    # Initialize the expected goals (xg)\n",
    "    xg_total = 0\n",
    "    count_matches = 0\n",
    "\n",
    "    # Season of the current match\n",
    "    current_season = row['Season']\n",
    "\n",
    "    # Date of the current match\n",
    "    current_date = pd.to_datetime(row['Date'], dayfirst=True)  # Ensure the date format is correct\n",
    "\n",
    "    # Define the opponent column based on the team column\n",
    "    if team_column == 'Team_ID':\n",
    "        goals_col = 'FTHG'\n",
    "        shots_on_target_col = 'HST'\n",
    "    else:\n",
    "        goals_col = 'FTAG'\n",
    "        shots_on_target_col = 'AST'\n",
    "\n",
    "    # Filter DataFrame for matches from the same season before the current date\n",
    "    past_matches = df[\n",
    "        (df['Season'] == current_season) &\n",
    "        (pd.to_datetime(df['Date'], dayfirst=True) < current_date) &\n",
    "        (df[team_column] == row[team_column])\n",
    "    ]\n",
    "\n",
    "    # Calculate efficiency and xg\n",
    "    for match in past_matches.itertuples():\n",
    "        goals = getattr(match, goals_col)\n",
    "        shots_on_target = getattr(match, shots_on_target_col)\n",
    "        if shots_on_target > 0:\n",
    "            efficiency = goals / shots_on_target\n",
    "            xg_total += efficiency\n",
    "            count_matches += 1\n",
    "\n",
    "    # Calculate average xg\n",
    "    if count_matches > 0:\n",
    "        avg_xg = xg_total / count_matches\n",
    "    else:\n",
    "        avg_xg = 0\n",
    "\n",
    "    return avg_xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['team_xg'] = df.apply(lambda x: calculate_xg(df, x, 'Team_ID')  if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)\n",
    "df['opp_xg'] = df.apply(lambda x: calculate_xg(df, x, 'Opp_ID') if dataprep_start_date is None or x['Date'] >= dataprep_start_date else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    "    \n",
    "        'Div', 'Season', 'Date_temp', 'Time', 'DayOTW', 'Team_ID', 'Opp_ID', 'FTR',\n",
    "\n",
    "        'team_elo', 'opp_elo',\n",
    "\n",
    "        'team_xg', 'opp_xg',\n",
    "        \n",
    "        'team_hist_vs', \n",
    "        'opp_hist_vs',\n",
    "\n",
    "        'team_points',\n",
    "        'opp_points',\n",
    "        \n",
    "        'team_form', \n",
    "        'opp_form',\n",
    "\n",
    "        'team_avg_goals_for', \n",
    "        'team_avg_goals_against',\n",
    "        'opp_avg_goals_for',\n",
    "        'opp_avg_goals_against',\n",
    "         \n",
    "        'team_shots', 'opp_shots',\n",
    "        'team_shots_target', 'opp_shots_target',\n",
    "\n",
    "        'AvgH', 'AvgD', 'AvgA'\n",
    "         \n",
    "         \n",
    "         ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_temp\n",
      "20240427    101\n",
      "20240428     52\n",
      "20240426     14\n",
      "20240429      7\n",
      "20150510      1\n",
      "20190317      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the value counts of the Date_temp column where FTR is -1\n",
    "print(df[df['FTR'] == -1]['Date_temp'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Date_temp' to 'Date'\n",
    "df.rename(columns={'Date_temp': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved: 56615 matches\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "   \n",
    "    if dataprep_start_date is not None:\n",
    "        # Convert date columns to datetime\n",
    "        df['Date_temp'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n",
    "        \n",
    "        # Filter new data based on start date\n",
    "        df_new = df[df['Date_temp'] >= dataprep_start_date].copy()\n",
    "\n",
    "        # Load existing data\n",
    "        df_existing = pd.read_csv(f'data/processed/processed_data_{content}.csv')\n",
    "\n",
    "        df_existing['Date_temp'] = pd.to_datetime(df_existing['Date'])\n",
    "        \n",
    "        # Filter existing data to remove overlap with new data\n",
    "        df_existing = df_existing[df_existing['Date_temp'] < dataprep_start_date]\n",
    "\n",
    "        # Combine and sort data\n",
    "        df_final = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_final.sort_values(['Date_temp', 'Time'], inplace=True)\n",
    "\n",
    "        # Clean up temporary columns\n",
    "        df_final.drop(columns='Date_temp', inplace=True)\n",
    "    else:\n",
    "        df_final = df.copy()\n",
    "\n",
    "    # Save the final DataFrame\n",
    "    df_final.to_csv(f'data/processed/processed_data_{content}.csv', index=False)\n",
    "    print(f\"Data saved: {df_final.shape[0]} matches\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 400  # Set Frequency To 2500 Hertz\n",
    "duration = 200  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler  # Assuming you might need it\n",
    "\n",
    "# Specific models and tools\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Encoding and feature selection\n",
    "from category_encoders import TargetEncoder  # Fixed the import based on usage\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Miscellaneous settings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"euro_simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data csv into a DataFrame\n",
    "df = pd.read_csv(f'data/processed/processed_data_{content}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the date_temp column, which is in YYYYMMDD format, into a datetime object, and store in a new column 'date_temporary'\n",
    "df['date_temporary'] = pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the current date dynamically\n",
    "date_today = pd.Timestamp.now().normalize()  # .normalize() sets the time to 00:00:00\n",
    "\n",
    "# Calculate the date 2 weeks ago from the current date\n",
    "date_2_weeks_ago = date_today - pd.DateOffset(days=14)\n",
    "\n",
    "date_cutoff = date_today - pd.DateOffset(days=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where the date_temporary column is older than date_cutoff\n",
    "df = df[df['date_temporary'] >= date_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_validationset = df.tail(250)\n",
    "#df = df.iloc[:-250]\n",
    "\n",
    "# define df_validationset as all the rows in df where the date_temporary column is greater than date_2_weeks_ago\n",
    "df_validationset = df[df['date_temporary'] > date_2_weeks_ago]\n",
    "\n",
    "# define df as all the rows in df where the date_temporary column is less than or equal to date_2_weeks_ago\n",
    "df = df[df['date_temporary'] <= date_2_weeks_ago]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11927, 234)"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df_validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the date_temporary column\n",
    "df.drop(columns=['date_temporary'], inplace=True)\n",
    "df_validationset.drop(columns=['date_temporary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "teams_dict = {}\n",
    "\n",
    "# Assuming 'teams_dict.txt' contains the dictionary as a single string\n",
    "with open(f'data/teams_dict_{content}.txt', 'r') as file:\n",
    "    # Read the entire file content into a single string\n",
    "    data = file.read()\n",
    "    # Safely evaluate the string as a Python dictionary\n",
    "    teams_dict = ast.literal_eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the df and df_validationset DataFrames by the 'Date', 'Div', 'Time' columns\n",
    "df.sort_values(['Date', 'Div', 'Time'], inplace=True)\n",
    "df_validationset.sort_values(['Date', 'Div', 'Time'], inplace=True)\n",
    "\n",
    "# Set the 'Date' and 'FTR2' column as the index\n",
    "df.set_index(['Date', 'FTR2'], inplace=True)\n",
    "df_validationset.set_index(['Date', 'FTR2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into X and y\n",
    "X = df.drop('FTR', axis=1)\n",
    "y = df['FTR']\n",
    "\n",
    "X.columns = [re.sub(r'[<]', '_st_', str(col)) for col in X.columns]\n",
    "X.columns = [re.sub(r'[>]', '_gt_', str(col)) for col in X.columns]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows(X, window_size, step):\n",
    "    n_samples = len(X)\n",
    "    windows = []\n",
    "    for start_idx in range(0, n_samples - window_size + 1, step):\n",
    "        end_idx = start_idx + window_size\n",
    "        if end_idx > n_samples:\n",
    "            break  # Avoid going beyond the dataset\n",
    "        train_indices = list(range(max(0, start_idx - window_size), start_idx))\n",
    "        test_indices = list(range(start_idx, end_idx))\n",
    "        windows.append((train_indices, test_indices))\n",
    "    return windows\n",
    "\n",
    "negative_count = len(df[df['FTR'] == 0])\n",
    "positive_count = len(df[df['FTR'] == 1])\n",
    "scale_pos_weight_value = negative_count / positive_count\n",
    "\n",
    "percentile = [50, 75, 100]\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "\n",
    "    #'xgb__select__percentile': [100],\n",
    "    #'gb__select__percentile': [100],\n",
    "    #'lr__select__percentile': [100],\n",
    "    #'ada__select__percentile': [100],\n",
    "    \n",
    "    'xgb__clf__max_depth': [1,2,3],\n",
    "    'xgb__clf__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'xgb__clf__lambda': [1, 1.5, 2],  # L2 regularization term on weights\n",
    "    'xgb__clf__alpha': [0, 0.5, 1],  # L1 regularization term on weights\n",
    "    'xgb__clf__n_estimators': [1, 5, 100],\n",
    "\n",
    "    'rf__clf__max_depth': [1, 2],\n",
    "    #'rf__clf__min_samples_split': [2, 5],\n",
    "    #'rf__clf__min_samples_leaf': [1, 2],\n",
    "    #'rf__clf__bootstrap': [True, False],\n",
    "    'rf__clf__n_estimators': [1, 5, 100],\n",
    "\n",
    "    'lr__clf__C': [0.1, 1],  # Inverse of regularization strength; smaller values specify stronger regularization.\n",
    "    'lr__clf__penalty': ['l1', 'l2', 'elasticnet'],  # Specify the norm of the penalty.\n",
    "    'lr__clf__solver': ['saga'],  # Algorithm to use in the optimization problem, 'saga' supports all penalties.\n",
    "    'lr__clf__l1_ratio': [0.5],  # The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if penalty='elasticnet'.\n",
    "    'lr__clf__class_weight': ['balanced'],\n",
    "\n",
    "    #'cat__clf__depth': [1,2,3,4],\n",
    "    #'cat__clf__learning_rate': [0.01, 0.05, 0.1],\n",
    "    #'cat__clf__iterations': [50, 100, 200],\n",
    "    #'cat__clf__l2_leaf_reg': [1, 3, 5],\n",
    "\n",
    "    'gb__clf__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb__clf__n_estimators': [50, 100, 200],\n",
    "    'gb__clf__max_depth': [3, 5 , 7],\n",
    "    'gb__clf__min_samples_split': [2, 5],\n",
    "    'gb__clf__min_samples_leaf': [1, 2],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "param_test = {\n",
    "    \n",
    "    'xgb__clf__max_depth': [1,2],\n",
    "    'xgb__clf__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'xgb__clf__lambda': [1, 1.5, 2],  # L2 regularization term on weights\n",
    "    'xgb__clf__alpha': [0, 0.5, 1],  # L1 regularization term on weights\n",
    "    'xgb__clf__n_estimators': [1, 5, 100],\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a custom scoring function\n",
    "def xgb_early_stopping_score(y, estimator, X, y_true, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Custom scorer that uses early stopping.\n",
    "    \"\"\"\n",
    "    # Split X into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_true, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit with early stopping\n",
    "    eval_set = [(X_val, y_val)]\n",
    "    estimator.fit(X_train, y_train, early_stopping_rounds=25, eval_set=eval_set, verbose=False)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = estimator.predict(X_val)\n",
    "    \n",
    "    # Return the F1 score\n",
    "    return f1_score(y_val, y_pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection\n",
    "from sklearn.feature_selection import SelectPercentile, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Training Data Shape: (596, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.7667785234899329\n",
      "\n",
      "Iteration 2 Training Data Shape: (1192, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6895973154362416\n",
      "\n",
      "Iteration 3 Training Data Shape: (1788, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6644295302013423\n",
      "\n",
      "Iteration 4 Training Data Shape: (2384, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.7164429530201343\n",
      "\n",
      "Iteration 5 Training Data Shape: (2980, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.62751677852349\n",
      "\n",
      "Iteration 6 Training Data Shape: (3576, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6795302013422819\n",
      "\n",
      "Iteration 7 Training Data Shape: (4172, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.639261744966443\n",
      "\n",
      "Iteration 8 Training Data Shape: (4768, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6140939597315436\n",
      "\n",
      "Iteration 9 Training Data Shape: (5364, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6191275167785235\n",
      "\n",
      "Iteration 10 Training Data Shape: (5960, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.610738255033557\n",
      "\n",
      "Iteration 11 Training Data Shape: (6556, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6426174496644296\n",
      "\n",
      "Iteration 12 Training Data Shape: (7152, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6459731543624161\n",
      "\n",
      "Iteration 13 Training Data Shape: (7748, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6157718120805369\n",
      "\n",
      "Iteration 14 Training Data Shape: (8344, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.639261744966443\n",
      "\n",
      "Iteration 15 Training Data Shape: (8940, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6124161073825504\n",
      "\n",
      "Iteration 16 Training Data Shape: (9536, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6291946308724832\n",
      "\n",
      "Iteration 17 Training Data Shape: (10132, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6526845637583892\n",
      "\n",
      "Iteration 18 Training Data Shape: (10728, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6342281879194631\n",
      "\n",
      "Iteration 19 Training Data Shape: (11324, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.6023489932885906\n",
      "\n",
      "Iteration 20 Training Data Shape: (11920, 9)\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Best Parameters: {'xgb__clf__n_estimators': 100, 'xgb__clf__max_depth': 3, 'xgb__clf__learning_rate': 0.1, 'xgb__clf__lambda': 1.5, 'xgb__clf__alpha': 1, 'rf__clf__n_estimators': 1, 'rf__clf__max_depth': 2, 'lr__clf__solver': 'saga', 'lr__clf__penalty': 'elasticnet', 'lr__clf__l1_ratio': 0.5, 'lr__clf__class_weight': 'balanced', 'lr__clf__C': 1, 'gb__clf__n_estimators': 100, 'gb__clf__min_samples_split': 2, 'gb__clf__min_samples_leaf': 1, 'gb__clf__max_depth': 5, 'gb__clf__learning_rate': 0.01}\n",
      "Precision: 0.62751677852349\n",
      "\n",
      "\n",
      "Best F1 Score: 0.6832504145936982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.58      0.65       353\n",
      "           1       0.53      0.70      0.60       243\n",
      "\n",
      "    accuracy                           0.63       596\n",
      "   macro avg       0.63      0.64      0.63       596\n",
      "weighted avg       0.65      0.63      0.63       596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, StratifiedKFold, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the F1 score for the '1' class\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "best_f1_score = 0\n",
    "best_f1_params = None\n",
    "best_window_size = None\n",
    "best_precision = 0\n",
    "best_model = None \n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "# Make the custom scorer\n",
    "custom_scorer = make_scorer(xgb_early_stopping_score, greater_is_better=True, needs_proba=False, X=X, y_true=y)\n",
    "\n",
    "# Set the window_size and step to 5% of the dataset\n",
    "window_size = int(len(X) * 0.05)\n",
    "step = int(len(X) * 0.05)\n",
    "\n",
    "# Initialize an empty list to store precision scores\n",
    "precision_scores = []\n",
    "\n",
    "# Initialize an empty dataframe to store misclassified samples\n",
    "misclassified_samples = pd.DataFrame(columns=X.columns)\n",
    "\n",
    "# Generate windows\n",
    "window_splits = generate_sliding_windows(X, window_size, step)\n",
    "\n",
    "# Initialize training indices with the first window\n",
    "train_end_index = window_size\n",
    "\n",
    "# Iterate over each sliding window\n",
    "for i, (train_index, test_index) in enumerate(window_splits):\n",
    "\n",
    "    # Update training indices to include the next window\n",
    "    train_index = list(range(train_end_index))\n",
    "    train_end_index += window_size\n",
    "\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "    print(f\"Iteration {i+1} Training Data Shape: {X_train.shape}\")\n",
    "\n",
    "    # Combine misclassified samples from previous iterations with current training data\n",
    "    if not misclassified_samples.empty:\n",
    "        X_train_combined = pd.concat([X_train, misclassified_samples[X_train.columns]], axis=0)\n",
    "        y_train_combined = pd.concat([y_train, misclassified_samples['FTR']], axis=0)\n",
    "    else:\n",
    "        X_train_combined = X_train\n",
    "        y_train_combined = y_train\n",
    "\n",
    "    # Calculate misclassification frequency\n",
    "    misclassified_freq = y_train_combined.value_counts(normalize=True)\n",
    "\n",
    "    # Define class weights based on misclassification frequency\n",
    "    class_weights = {0: 1, 1: max(0.6, 5 - misclassified_freq.get(1, 0.5))}  # Adjust dynamically to penalize misclassification of class 1 more heavily\n",
    "\n",
    "    pct = 50\n",
    "\n",
    "    # Define pipelines for each classifier with SMOTE and TargetEncoder\n",
    "    pipeline_xgb = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', XGBClassifier(random_state=42, verbose=0))\n",
    "    ])\n",
    "\n",
    "    pipeline_gb = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', GradientBoostingClassifier(random_state=42, verbose=0))\n",
    "    ])\n",
    "\n",
    "    # pipeline for logistic regression\n",
    "    pipeline_lr = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', LogisticRegression(random_state=42, verbose=0))\n",
    "    ])\n",
    "\n",
    "    # pipeline for catboost classifier\n",
    "    pipeline_cat = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', CatBoostClassifier(random_state=42, verbose=0))\n",
    "    ])\n",
    "\n",
    "    # pipeline for random forest\n",
    "    pipeline_rf = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', RandomForestClassifier(random_state=42, verbose=0))\n",
    "    ])\n",
    "\n",
    "    # LightGBM pipeline\n",
    "    pipeline_lgbm = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', LGBMClassifier(random_state=42, force_col_wise='true', verbose=0))\n",
    "    ])\n",
    "\n",
    "    # Adaboost pipeline\n",
    "    pipeline_ada = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', AdaBoostClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Support Vector Machine pipeline\n",
    "    pipeline_svm = ImbPipeline([\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('select', SelectPercentile(chi2, percentile=pct)),\n",
    "        ('clf', SVC(random_state=42, probability=True, verbose=0))\n",
    "\n",
    "    ])\n",
    "\n",
    "    # Combine them into an ensemble classifier\n",
    "    ensemble_clf = VotingClassifier(estimators=[\n",
    "        ('xgb', pipeline_xgb),\n",
    "        ('gb', pipeline_gb),\n",
    "        ('lr', pipeline_lr),\n",
    "        #('cat', pipeline_cat),\n",
    "        ('rf', pipeline_rf),\n",
    "        #('lgbm', pipeline_lgbm),\n",
    "        #('ada', pipeline_ada)\n",
    "        #('svm', pipeline_svm)\n",
    "    ], voting='soft')\n",
    "\n",
    "    # Setup RandomizedSearchCV\n",
    "    clf = RandomizedSearchCV(\n",
    "        estimator=ensemble_clf,\n",
    "        param_distributions=param_dist,\n",
    "        #param_distributions=param_test,\n",
    "        n_iter=2,\n",
    "        scoring=custom_scorer,\n",
    "        cv=TimeSeriesSplit(n_splits=2),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )  \n",
    "\n",
    "\n",
    "    # Fit RandomizedSearchCV\n",
    "    clf.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = clf.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # Use the best estimator\n",
    "    best_pipe = clf.best_estimator_\n",
    "\n",
    "    # Make predictions\n",
    "    y_proba = best_pipe.predict_proba(X_test)\n",
    "\n",
    "    # Apply threshold\n",
    "    threshold = 0.5  # You can adjust this threshold as needed\n",
    "    y_pred = (y_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "    current_f1_score = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    if current_f1_score > best_f1_score:\n",
    "        best_f1_score = current_f1_score\n",
    "        best_f1_params = clf.best_params_\n",
    "        #best_model = clf.best_estimator_ \n",
    "\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    best_model = clf.best_estimator_ \n",
    "\n",
    "    # Calculate precision score\n",
    "    precision = np.mean(y_test == y_pred)\n",
    "    precision_scores.append(precision)\n",
    "    print(\"Precision:\", precision)\n",
    "\n",
    "    print()\n",
    "\n",
    "# Print the best F1 score and its corresponding parameters\n",
    "print()\n",
    "print(\"Best F1 Score:\", best_f1_score)\n",
    "\n",
    "# print the classification report of the best model on the full dataset\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Put the target column to the front\n",
    "cols = list(corr.columns)\n",
    "cols.insert(0, cols.pop(cols.index('FTR')))\n",
    "corr = corr.loc[cols, cols]\n",
    "\n",
    "# Plot the correlation matrix\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=2)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature importances\n",
    "#importances = search.best_estimator_.named_steps['xgb'].feature_importances_\n",
    "#features = X_train.columns\n",
    "#importances_df = pd.DataFrame({'features': features, 'importances': importances})\n",
    "#importances_df = importances_df.sort_values('importances', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.barplot(x='importances', y='features', data=importances_df)\n",
    "#plt.title('Feature Importances')\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_value_bet(row):\n",
    "    \"\"\"\n",
    "    Calculate the value bet based on the model's probability and the bookmaker's odds.\n",
    "    \n",
    "    Parameters:\n",
    "    - row: A row from a DataFrame, containing the 'Probability' and 'AvgH' columns.\n",
    "    \n",
    "    Returns:\n",
    "    - The calculated value of the bet.\n",
    "    \"\"\"\n",
    "    decimal_odds = row['AvgH']\n",
    "    model_probability = row['Probability']\n",
    "    value = (decimal_odds * model_probability) - 1\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.5640000000000001\n",
      "Best Accuracy: 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "df_val = df_validationset.copy()\n",
    "\n",
    "# Calculate the predicted probabilities for the validation set\n",
    "y_val_proba = best_model.predict_proba(df_val.drop(columns=['FTR']))\n",
    "\n",
    "# Initialize variables to track the best threshold and its corresponding accuracy\n",
    "best_threshold = 0.5\n",
    "best_accuracy = 0\n",
    "\n",
    "# Iterate over potential threshold values\n",
    "for threshold in np.arange(0.5, 0.85, 0.001):\n",
    "    # Apply the current threshold to generate predictions\n",
    "    y_val_pred = (y_val_proba[:, 1] >= threshold).astype(int)\n",
    "    \n",
    "    # Evaluate accuracy for the current set of predictions\n",
    "    accuracy = accuracy_score(df_val['FTR'], y_val_pred)\n",
    "    \n",
    "    # Update the best threshold and accuracy as needed\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_threshold = threshold \n",
    "\n",
    "# Print the best threshold and its accuracy\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Apply the best threshold to generate the final set of predictions\n",
    "y_val_pred_best = (y_val_proba[:, 1] >= best_threshold).astype(int)\n",
    "\n",
    "# Add the prediction probabilities and final predictions to df_val\n",
    "#df_val['proba_0'] = y_val_proba[:, 0].round(3)\n",
    "df_val['Probability'] = y_val_proba[:, 1].round(3)\n",
    "df_val['Prediction'] = y_val_pred_best\n",
    "\n",
    "\n",
    "# Directly filter df_val and add necessary columns without separate reindexing steps\n",
    "#filtered_df_val = df_val[df_val['Probability'] > best_threshold].copy()\n",
    "\n",
    "# Display all predictions\n",
    "filtered_df_val = df_val.copy()\n",
    "\n",
    "filtered_df_val.reset_index(inplace=True)\n",
    "\n",
    "filtered_df_val['Prediction'] = (filtered_df_val['Probability'] >= best_threshold).astype(int)\n",
    "#filtered_df_val['Actual Result'] = filtered_df_val['FTR']\n",
    "filtered_df_val['Correct Prediction'] = (filtered_df_val['Prediction'] == filtered_df_val['FTR']).astype(bool)\n",
    "\n",
    "index_to_team = {v: k for k, v in teams_dict.items()}\n",
    "filtered_df_val['Team'] = filtered_df_val['Team_ID'].map(index_to_team)\n",
    "filtered_df_val['Opponent'] = filtered_df_val['Opp_ID'].map(index_to_team)\n",
    "\n",
    "# Apply the calculate_value_bet function to each row in filtered_df_val to calculate the 'value bet'\n",
    "filtered_df_val['Value Bet'] = filtered_df_val.apply(calculate_value_bet, axis=1).round(2)\n",
    "\n",
    "display_columns = [\n",
    "    'Div', 'Date', 'Time', 'Team', 'Opponent', 'Probability', 'Prediction',\n",
    "    #'FTR', \n",
    "    'FTR2', 'Correct Prediction', 'AvgH', 'AvgD', 'AvgA', 'Value Bet'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = filtered_df_val[display_columns]\n",
    "\n",
    "output = output.sort_values('Date', ascending=False)\n",
    "\n",
    "#output = output[output['Probability'] > best_threshold].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by Div, Date, Time, Team\n",
    "output.sort_values(['Div', 'Date', 'Team'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Div</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Team</th>\n",
       "      <th>Opponent</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>FTR2</th>\n",
       "      <th>Correct Prediction</th>\n",
       "      <th>AvgH</th>\n",
       "      <th>AvgD</th>\n",
       "      <th>AvgA</th>\n",
       "      <th>Value Bet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20240330</td>\n",
       "      <td>17</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>0.727</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1.40</td>\n",
       "      <td>5.71</td>\n",
       "      <td>6.47</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20240330</td>\n",
       "      <td>14</td>\n",
       "      <td>Ein Frankfurt</td>\n",
       "      <td>Union Berlin</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.04</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.81</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20240330</td>\n",
       "      <td>14</td>\n",
       "      <td>M'gladbach</td>\n",
       "      <td>Freiburg</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20240330</td>\n",
       "      <td>14</td>\n",
       "      <td>RB Leipzig</td>\n",
       "      <td>Mainz</td>\n",
       "      <td>0.743</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.38</td>\n",
       "      <td>5.30</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20240330</td>\n",
       "      <td>14</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>Wolfsburg</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>11</td>\n",
       "      <td>20240407</td>\n",
       "      <td>13</td>\n",
       "      <td>Huesca</td>\n",
       "      <td>Leganes</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.67</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>11</td>\n",
       "      <td>20240407</td>\n",
       "      <td>15</td>\n",
       "      <td>Mirandes</td>\n",
       "      <td>Sp Gijon</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.92</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>11</td>\n",
       "      <td>20240407</td>\n",
       "      <td>17</td>\n",
       "      <td>Tenerife</td>\n",
       "      <td>Eibar</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>11</td>\n",
       "      <td>20240407</td>\n",
       "      <td>15</td>\n",
       "      <td>Villarreal B</td>\n",
       "      <td>Burgos</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.04</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>11</td>\n",
       "      <td>20240408</td>\n",
       "      <td>19</td>\n",
       "      <td>Santander</td>\n",
       "      <td>Alcorcon</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.48</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Div      Date  Time           Team      Opponent  Probability  \\\n",
       "5      0  20240330    17  Bayern Munich      Dortmund        0.727   \n",
       "0      0  20240330    14  Ein Frankfurt  Union Berlin        0.556   \n",
       "2      0  20240330    14     M'gladbach      Freiburg        0.531   \n",
       "3      0  20240330    14     RB Leipzig         Mainz        0.743   \n",
       "4      0  20240330    14  Werder Bremen     Wolfsburg        0.366   \n",
       "..   ...       ...   ...            ...           ...          ...   \n",
       "211   11  20240407    13         Huesca       Leganes        0.480   \n",
       "212   11  20240407    15       Mirandes      Sp Gijon        0.428   \n",
       "215   11  20240407    17       Tenerife         Eibar        0.417   \n",
       "213   11  20240407    15   Villarreal B        Burgos        0.395   \n",
       "219   11  20240408    19      Santander      Alcorcon        0.585   \n",
       "\n",
       "     Prediction  FTR2  Correct Prediction  AvgH  AvgD  AvgA  Value Bet  \n",
       "5             1     2               False  1.40  5.71  6.47       0.02  \n",
       "0             0     0                True  2.04  3.44  3.81       0.13  \n",
       "2             0     2                True  2.23  3.68  3.11       0.18  \n",
       "3             1     0               False  1.38  5.30  7.65       0.03  \n",
       "4             0     2                True  2.84  3.56  2.43       0.04  \n",
       "..          ...   ...                 ...   ...   ...   ...        ...  \n",
       "211           0     0                True  2.43  2.67  3.54       0.17  \n",
       "212           0     2                True  2.52  2.92  3.00       0.08  \n",
       "215           0     1               False  2.54  2.90  3.05       0.06  \n",
       "213           0     1               False  2.98  3.04  2.46       0.18  \n",
       "219           1     2               False  1.75  3.48  4.75       0.02  \n",
       "\n",
       "[204 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output[(output['Value Bet'] >= 0.01) & (output['AvgH'] >= 1.35)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.56\n",
      "Best Accuracy: 0.72\n",
      "\n",
      "Total Predictions: 234\n",
      "Total Correct Predictions: 169\n",
      "\n",
      "Percentage of Correct Predictions: 72.22%\n"
     ]
    }
   ],
   "source": [
    "# Display the Correct Prediction True / False ratio, and ther percentage of correct predictions\n",
    "correct_predictions = output['Correct Prediction'].sum()\n",
    "total_predictions = len(output)\n",
    "correct_ratio = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.2f}\")\n",
    "print()\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Total Correct Predictions: {correct_predictions}\")\n",
    "print()\n",
    "print(f\"Percentage of Correct Predictions: {correct_ratio * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Format the current date and time as a string\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save filtered_df_val[display_columns] to a CSV file\n",
    "output.to_csv(f'data/predictions/predictions_{content}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 400  # Set Frequency To 2500 Hertz\n",
    "duration = 200  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

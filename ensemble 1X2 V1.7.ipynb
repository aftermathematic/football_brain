{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Preprocessing and model selection tools\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold, GridSearchCV,\n",
    "                                     RandomizedSearchCV, TimeSeriesSplit)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Metrics and scoring\n",
    "from sklearn.metrics import (balanced_accuracy_score, classification_report, f1_score,\n",
    "                             make_scorer, confusion_matrix, precision_score, accuracy_score)\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier,\n",
    "                              AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, HistGradientBoostingClassifier)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Neural networks\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# selection\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectKBest\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Advanced models and ensemble techniques\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Iputing missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Handling imbalanced datasets\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Encoding and feature selection\n",
    "from category_encoders import TargetEncoder\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# Miscellaneous settings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"allcomps_3s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data csv into a DataFrame\n",
    "df = pd.read_csv(f'data/processed/processed_data_{content}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the date_temp column, which is in YYYYMMDD format, into a datetime object, and store in a new column 'date_temporary'\n",
    "df['date_temporary'] = pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Div', 'Season', 'Date', 'Time', 'DayOTW', 'Team_ID', 'Opp_ID', 'FTR',\n",
       "       'team_elo', 'opp_elo', 'team_xg', 'opp_xg', 'team_hist_vs',\n",
       "       'opp_hist_vs', 'team_points', 'opp_points', 'elo_diff', 'xg_diff',\n",
       "       'points_diff', 'form_diff', 'team_form', 'opp_form',\n",
       "       'team_avg_goals_for', 'team_avg_goals_against', 'opp_avg_goals_for',\n",
       "       'opp_avg_goals_against', 'team_shots', 'opp_shots', 'team_shots_target',\n",
       "       'opp_shots_target', 'AvgH', 'AvgD', 'AvgA', 'date_temporary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "df.drop(columns=[ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#'AvgH', 'AvgD', 'AvgA',\n",
    "\n",
    "#'Season', 'Div', 'DayOTW',\n",
    "\n",
    "#'team_xg', 'opp_xg', \n",
    "#'team_hist_vs', 'opp_hist_vs', \n",
    "\n",
    "#'team_points', 'opp_points', \n",
    "#'team_form', 'opp_form',\n",
    "\n",
    "#'team_shots', 'opp_shots', \n",
    "#'team_shots_target', 'opp_shots_target',\n",
    "\n",
    "#'team_avg_goals_against', 'opp_avg_goals_against',\n",
    "\n",
    " ], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the current date dynamically\n",
    "date_today = pd.Timestamp.now().normalize()  # .normalize() sets the time to 00:00:00\n",
    "\n",
    "# Declare a date by setting day, month, and year\n",
    "date_specific = pd.Timestamp(year=2024, month=5, day=24)\n",
    "\n",
    "# Calculate the date 2 weeks ago from the current date\n",
    "date_delta = date_specific - pd.DateOffset(days=10)\n",
    "\n",
    "# Specific start date\n",
    "#date_start = date_specific - pd.DateOffset(days=1000)\n",
    "\n",
    "# No filter, all data\n",
    "date_start = pd.Timestamp(year=2022, month=7, day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where the date_temporary column is older than date_start\n",
    "df = df[df['date_temporary'] >= date_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define df_validationset as all the rows in df where the date_temporary column is greater than date_delta\n",
    "df_validationset = df[df['date_temporary'] > date_delta]\n",
    "\n",
    "# define df as all the rows in df where the date_temporary column is less than or equal to date_delta\n",
    "df = df[df['date_temporary'] <= date_delta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14752, 329)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df), len(df_validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the date_temporary column\n",
    "df.drop(columns=['date_temporary'], inplace=True)\n",
    "df_validationset.drop(columns=['date_temporary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "teams_dict = {}\n",
    "comps_dict = {}\n",
    "\n",
    "with open(f'data/teams_dict_{content}.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    teams_dict = ast.literal_eval(data)\n",
    "\n",
    "with open(f'data/comps_dict_{content}.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "    comps_dict = ast.literal_eval(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the df and df_validationset DataFrames by the 'Date', 'Div', 'Time' columns\n",
    "df.sort_values(['Date', 'Div', 'Time'], inplace=True)\n",
    "df_validationset.sort_values(['Date', 'Div', 'Time'], inplace=True)\n",
    "\n",
    "# Set the 'Date' and 'FTR' column as the index\n",
    "df.set_index(['Date'], inplace=True)\n",
    "df_validationset.set_index(['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into X and y\n",
    "X = df.drop(['FTR', 'AvgH', 'AvgD', 'AvgA'], axis=1) \n",
    "y = df['FTR']\n",
    "\n",
    "X.columns = [re.sub(r'[<]', '_st_', str(col)) for col in X.columns]\n",
    "X.columns = [re.sub(r'[>]', '_gt_', str(col)) for col in X.columns]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "\n",
    "    'xgb__clf__max_depth': [1,2,3, 4, 6],\n",
    "    'xgb__clf__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'xgb__clf__reg_lambda': [0.01, 0.1], \n",
    "    'xgb__clf__alpha': [0, 0.5, 1],  \n",
    "    'xgb__clf__colsample_bytree': [0.7, 0.9],\n",
    "    'xgb__clf__subsample': [0.75, 0.85],\n",
    "    'xgb__clf__n_estimators': [1, 5, 10],\n",
    "\n",
    "    'rf__clf__max_depth': [1, 2],\n",
    "    'rf__clf__min_samples_split': [3, 6],\n",
    "    'rf__clf__min_samples_leaf': [1, 3],\n",
    "    'rf__clf__n_estimators': [1, 5, 10],\n",
    "    'rf__clf__max_features': ['sqrt', 'log2'],\n",
    "\n",
    "    'lr__clf__C': [0.1, 1],\n",
    "    'lr__clf__penalty': ['l1', 'l2', 'elasticnet'], \n",
    "    'lr__clf__solver': ['saga'], \n",
    "    'lr__clf__l1_ratio': [0.5], \n",
    "    'lr__clf__class_weight': ['balanced'],\n",
    "\n",
    "    'gb__clf__learning_rate': [0.01, 0.1, 0.15],\n",
    "    'gb__clf__n_estimators': [1, 5, 10],\n",
    "    'gb__clf__max_depth': [3, 5 , 7],\n",
    "    'gb__clf__min_samples_split': [2, 5],\n",
    "    'gb__clf__min_samples_leaf': [1, 2],\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_windows_by_div(X):\n",
    "    grouped = X.groupby('Div')  # Group by 'Div' only\n",
    "    windows = []\n",
    "\n",
    "    for div, group in grouped:\n",
    "        indices = group.index.tolist()  # Get indices of each group\n",
    "        n_samples = len(indices)\n",
    "\n",
    "        if n_samples < 2:\n",
    "            print(f\"Skipping division {div} with insufficient samples: {n_samples}\")\n",
    "            continue\n",
    "\n",
    "        # Using 80% of data for training and the rest for testing\n",
    "        split_point = int(n_samples * 0.8)\n",
    "        train_indices = indices[:split_point]\n",
    "        test_indices = indices[split_point:]\n",
    "\n",
    "        if train_indices and test_indices:  # Ensure both are non-empty\n",
    "            windows.append((train_indices, test_indices))\n",
    "\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def xgb_early_stopping_score(estimator, X, y_true, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Custom scorer that uses early stopping.\n",
    "    \"\"\"\n",
    "    # Predict using the already fitted estimator\n",
    "    y_pred = estimator.predict(X)\n",
    "    \n",
    "    # Return the F1 score or any other relevant metric\n",
    "    return f1_score(y_true, y_pred, pos_label=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(base_estimator, fts=14, random_state=42):\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('target_encoder', TargetEncoder()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        #('smote', SMOTE(random_state=random_state, k_neighbors=2)),\n",
    "        #('select', SelectKBest(chi2, k=fts)),\n",
    "        ('clf', clone(base_estimator))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'xgb': XGBClassifier(random_state=42, verbose=0),\n",
    "    'gb': GradientBoostingClassifier(random_state=42, verbose=0),\n",
    "    'lr': LogisticRegression(random_state=42, verbose=0, multi_class='ovr'),\n",
    "    'rf': RandomForestClassifier(random_state=42, verbose=0),\n",
    "    'ada': AdaBoostClassifier(random_state=42),\n",
    "    'lgbm': LGBMClassifier(random_state=42, force_col_wise='true', verbose=0),\n",
    "    'MLP': MLPClassifier(random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "# Generate pipelines for each classifier\n",
    "pipelines = {name: create_pipeline(clf) for name, clf in classifiers.items()}\n",
    "\n",
    "# Create the ensemble classifier\n",
    "ensemble_clf = VotingClassifier(\n",
    "    estimators=[(name, pipeline) for name, pipeline in pipelines.items()],\n",
    "    voting='soft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    'f1_score': make_scorer(f1_score, average='macro'),  \n",
    "    'accuracy': make_scorer(balanced_accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='macro'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_rolling_window_ensemble(X, y, window_size, step_size):\n",
    "    num_samples = len(X)\n",
    "    start_index = 0\n",
    "    # Initialize with an empty 'y' column\n",
    "    additional_training_data = pd.DataFrame(columns=['y'])\n",
    "\n",
    "    counter = 1\n",
    "\n",
    "    while start_index + window_size < num_samples:\n",
    "        end_index = start_index + window_size\n",
    "        \n",
    "        # Drop the 'y' column safely with errors='ignore' to handle cases where it might be absent\n",
    "        X_train = pd.concat([X.iloc[start_index:end_index], additional_training_data.drop(columns=['y'], errors='ignore')])\n",
    "        # Concatenate y-values safely\n",
    "        if not additional_training_data.empty:     \n",
    "\n",
    "            y_train = pd.concat([y.iloc[start_index:end_index], additional_training_data['y']])\n",
    "\n",
    "            print(f\"Additional data: {len(additional_training_data)}\")\n",
    "            print(f\"train size: {len(y_train)}\")\n",
    "        else:\n",
    "            y_train = y.iloc[start_index:end_index]\n",
    "        \n",
    "        X_test = X.iloc[end_index:end_index + step_size]\n",
    "        y_test = y.iloc[end_index:end_index + step_size]  \n",
    "\n",
    "        print()\n",
    "        print(f\"Iteration {counter}: Training on matches {start_index} to {end_index} of {num_samples}\")   \n",
    "\n",
    "\n",
    "        clf = RandomizedSearchCV(\n",
    "            estimator=ensemble_clf,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=10,\n",
    "            scoring=scoring,\n",
    "            refit='f1_score', \n",
    "            cv=TimeSeriesSplit(n_splits=10),\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            error_score='raise'\n",
    "        )\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        wrong_indices = y_test != y_pred\n",
    "        wrong_data = X_test[wrong_indices].copy()\n",
    "        wrong_data['y'] = y_test[wrong_indices]\n",
    "        additional_training_data = wrong_data\n",
    "\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "        start_index += step_size\n",
    "\n",
    "        counter += 1\n",
    "    \n",
    "    return clf  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 14752 \n",
      "\n",
      "Iteration 1: Training on matches 0 to 5000 of 14752\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Accuracy: 0.577\n",
      "Additional data: 423\n",
      "train size: 5423\n",
      "\n",
      "Iteration 2: Training on matches 1000 to 6000 of 14752\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Accuracy: 0.585\n",
      "Additional data: 415\n",
      "train size: 5415\n",
      "\n",
      "Iteration 3: Training on matches 2000 to 7000 of 14752\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Warning: No categorical columns found. Calling 'transform' will only return input data.\n",
      "Accuracy: 0.539\n",
      "Additional data: 461\n",
      "train size: 5461\n",
      "\n",
      "Iteration 4: Training on matches 3000 to 8000 of 14752\n"
     ]
    }
   ],
   "source": [
    "# Make the custom scorer\n",
    "custom_scorer = make_scorer(xgb_early_stopping_score, greater_is_better=True, needs_proba=False, X=X, y_true=y)\n",
    "\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of rows: {len(X)} \") \n",
    "\n",
    "window_size = 5000  \n",
    "step_size =  1000\n",
    "model = enhanced_rolling_window_ensemble(X, y, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRint a classification report\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "\n",
    "# Initialize a dictionary to store feature importances\n",
    "feature_importances = {}\n",
    "\n",
    "# Loop through each classifier in the ensemble\n",
    "for clf_name, clf_pipeline in best_model.named_estimators_.items():\n",
    "    if hasattr(clf_pipeline.named_steps['clf'], 'feature_importances_'):\n",
    "        # Extract feature importances\n",
    "        importances = clf_pipeline.named_steps['clf'].feature_importances_\n",
    "\n",
    "        # Access feature names via the 'select' step in pipeline if available\n",
    "        # Assuming feature selection might alter the features passed to the classifier\n",
    "        if 'select' in clf_pipeline.named_steps:\n",
    "            mask = clf_pipeline.named_steps['select'].get_support()  # Get the boolean mask\n",
    "            feature_names = np.array(X.columns)[mask]\n",
    "        else:\n",
    "            feature_names = np.array(X.columns)\n",
    "\n",
    "        # Combine feature names and their corresponding importance\n",
    "        feature_importances[clf_name] = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# Now plot the feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "avg_importances = pd.DataFrame(feature_importances).mean(axis=1).sort_values(ascending=False)\n",
    "\n",
    "avg_importances.plot(kind='bar')\n",
    "plt.title('Average Feature Importances Across Ensemble Models')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_validationset.copy()\n",
    "\n",
    "# Calculate the predicted probabilities for the validation set\n",
    "y_val_proba = best_model.predict_proba(df_val.drop(columns=['FTR', 'AvgH', 'AvgD', 'AvgA']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_val), len(y_val_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['Prob1'] = y_val_proba[:, 0].round(3)\n",
    "df_val['ProbX'] = y_val_proba[:, 1].round(3)\n",
    "df_val['Prob2'] = y_val_proba[:, 2].round(3)\n",
    "\n",
    "# Get the column index of the y_val_proba with the highest probability\n",
    "df_val['Prediction'] = y_val_proba.argmax(axis=1)\n",
    "\n",
    "# Map the prediction column to the actual result\n",
    "df_val['Prediction'] = df_val['Prediction'].map({0: '1', 1: 'X', 2: '2'})\n",
    "\n",
    "# Display all predictions\n",
    "filtered_df_val = df_val.copy()\n",
    "\n",
    "filtered_df_val.reset_index(inplace=True)\n",
    "\n",
    "# Map the 'Team_ID' and 'Opp_ID' columns to the actual team names\n",
    "index_to_team = {v: k for k, v in teams_dict.items()}\n",
    "filtered_df_val['Team'] = filtered_df_val['Team_ID'].map(index_to_team)\n",
    "filtered_df_val['Opponent'] = filtered_df_val['Opp_ID'].map(index_to_team)\n",
    "\n",
    "# Map the 'Div' column to the actual competition name\n",
    "index_to_comp = {v: k for k, v in comps_dict.items()}\n",
    "filtered_df_val['Div'] = filtered_df_val['Div'].map(index_to_comp)\n",
    "\n",
    "display_columns = [\n",
    "    'Div', \n",
    "    \n",
    "    'Date', 'Time', 'Team', 'Opponent', \n",
    "    \n",
    "    'FTR',\n",
    "\n",
    "    'team_points', 'opp_points',\n",
    "    'team_form', 'opp_form',\n",
    "    \n",
    "    'Prediction', \n",
    "    \n",
    "    'team_elo',     \n",
    "    'opp_elo',  \n",
    "    'elo_diff',\n",
    "\n",
    "    'team_xg', 'opp_xg',\n",
    "\n",
    "    'Prob1', 'ProbX', 'Prob2',    \n",
    "       \n",
    "    'AvgH', 'AvgD', 'AvgA', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = filtered_df_val[display_columns]\n",
    "\n",
    "output.sort_values(['Div', 'Date', 'Team'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(row):\n",
    "\n",
    "    prob1 = row['Prob1']\n",
    "    probX = row['ProbX']\n",
    "    prob2 = row['Prob2']\n",
    "\n",
    "    # Directly return '1' or '2' if their probabilities are greater than 0.6\n",
    "    if prob1 > 0.65:\n",
    "        return '1'\n",
    "    if prob2 > 0.65:\n",
    "        return '2'    \n",
    "\n",
    "    # Define the expected value (probability * bookmaker's odds)\n",
    "    # Calculate combined probabilities for '1X' and 'X2'\n",
    "    prob1X = prob1 + probX\n",
    "    probX2 = probX + prob2\n",
    "\n",
    "    # Create a dictionary to compare probabilities with bet types\n",
    "    bets = {\n",
    "        '1': prob1,\n",
    "        'X': probX,\n",
    "        '2': prob2,\n",
    "        '1X': prob1X,\n",
    "        'X2': probX2\n",
    "    }\n",
    "\n",
    "    # Determine the best bet by finding the maximum probability\n",
    "    best_bet = max(bets, key=bets.get)\n",
    "    return best_bet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['1X2'] = output.apply(make_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bet_correct(row):\n",
    "    if row['FTR'] == 0:\n",
    "        return row['1X2'] in ['1', '1X']\n",
    "    elif row['FTR'] == 1:\n",
    "        return row['1X2'] in ['1X', 'X', 'X2']\n",
    "    elif row['FTR'] == 2:\n",
    "        return row['1X2'] in ['X2', '2']\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output['FTR'] is not None:\n",
    "    output['Correct'] = output.apply(is_bet_correct, axis=1)\n",
    "else:\n",
    "    output['Correct'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_confidence(row):\n",
    "\n",
    "    Prob1 = row['Prob1']\n",
    "    ProbX = row['ProbX']\n",
    "    Prob2 = row['Prob2']\n",
    "\n",
    "    # parse the probabilities into floats\n",
    "    Prob1 = float(Prob1)\n",
    "    ProbX = float(ProbX)\n",
    "    Prob2 = float(Prob2)\n",
    "\n",
    "    conf = 0\n",
    "\n",
    "    if row['1X2'] == '1':\n",
    "        conf =  Prob1\n",
    "    elif row['1X2'] == '1X':\n",
    "        conf = Prob1 + ProbX\n",
    "    elif row['1X2'] == 'X':\n",
    "        conf = ProbX\n",
    "    elif row['1X2'] == 'X2':\n",
    "        conf = ProbX + Prob2\n",
    "    elif row['1X2'] == '2':\n",
    "        conf =  Prob2\n",
    "\n",
    "    # round the confidence to 2 decimal places\n",
    "    conf = round(conf, 2)\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Confidence'] = output.apply(bet_confidence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_bet(row):\n",
    "\n",
    "    Prob1 = float(row['Prob1'])\n",
    "    ProbX = float(row['ProbX'])\n",
    "    Prob2 = float(row['Prob2'])\n",
    "\n",
    "    AvgH = float(row['AvgH'])\n",
    "    AvgD = float(row['AvgD'])\n",
    "    AvgA = float(row['AvgA'])\n",
    "\n",
    "    # Calculate the probabilities of each outcome\n",
    "    OddProb1 = 1 / AvgH\n",
    "    OddProbX = 1 / AvgD\n",
    "    OddProb2 = 1 / AvgA\n",
    "\n",
    "    if row['1X2'] in ['1', '1X'] and Prob1 > OddProb1:\n",
    "        return True\n",
    "    elif row['1X2'] in ['X2', '2'] and Prob2 > OddProb2:\n",
    "        return True\n",
    "    elif row['1X2'] == 'X' and ProbX > OddProbX:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['Value'] = output.apply(value_bet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the 'FTR' back to the actual result\n",
    "output['FTR'] = output['FTR'].map({0: '1', 1: 'X', 2: '2'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['1', 'X', '2']\n",
    "\n",
    "# Filter the DataFrame based on the 'FTR' column and count the rows\n",
    "total_rows = output[output['FTR'].isin(values)].shape[0]\n",
    "\n",
    "# Display the total amount of predictions where the value is True\n",
    "#total_correct = output['Correct'].sum()\n",
    "total_correct = output[(output['FTR'].isin(values)) & (output['Correct'])].shape[0]\n",
    "\n",
    "# Calculate the percentage of correct predictions\n",
    "correct_percentage = (total_correct / total_rows) * 100\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total Rows: {total_rows}\")\n",
    "print(f\"Total Correct Predictions: {total_correct}\")\n",
    "print(f\"Percentage of Correct Predictions: {correct_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the most interesting matches to bet on\n",
    "\n",
    "# Timestamp\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Format the current date and time as a string\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Keep only the rows where 'FTR' column is null, '1X2' is 1 or 1X, and 'AvgH' is greater than 2\n",
    "interesting_matches_1 = output[(output['FTR'].isnull()) & (output['1X2'].isin(['1', '1X'])) & (output['AvgH'] > 2)]\n",
    "\n",
    "# Keep only the rows where 'FTR' column is null, '1X2' is 2 or X2, and 'AvgA' is greater than 3\n",
    "interesting_matches_2 = output[(output['FTR'].isnull()) & (output['1X2'].isin(['X2', '2'])) & (output['AvgA'] > 3)]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "interesting_matches = pd.concat([interesting_matches_1, interesting_matches_2])\n",
    "\n",
    "interesting_matches.to_csv(f'data/predictions/interesting_matches_{content}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if AvgH exists\n",
    "if 'AvgH' in output.columns:\n",
    "\n",
    "    # Change the decimal sign to a point for AvgH, AvgD, and AvgA columns to avoid parsing issues\n",
    "    output['AvgH'] = output['AvgH'].apply(lambda x: str(x).replace(',', '.'))\n",
    "    output['AvgD'] = output['AvgD'].apply(lambda x: str(x).replace(',', '.'))\n",
    "    output['AvgA'] = output['AvgA'].apply(lambda x: str(x).replace(',', '.'))\n",
    "\n",
    "    # parse AvgH, AvgD, AvgA columns as float\n",
    "    output['AvgH'] = output['AvgH'].astype(float)\n",
    "    output['AvgD'] = output['AvgD'].astype(float)\n",
    "    output['AvgA'] = output['AvgA'].astype(float)\n",
    "\n",
    "    # parse team_xg, opp_xg, team_form, opp_form, team_points, opp_points as float and round to 3 decimal places\n",
    "    output['team_xg'] = output['team_xg'].astype(float).round(3)\n",
    "    output['opp_xg'] = output['opp_xg'].astype(float).round(3)\n",
    "    output['team_form'] = output['team_form'].astype(float).round(3)\n",
    "    output['opp_form'] = output['opp_form'].astype(float).round(3)\n",
    "    output['team_points'] = output['team_points'].astype(float).round(3)\n",
    "    output['opp_points'] = output['opp_points'].astype(float).round(3)\n",
    "\n",
    "    # Change the decimal sign to a comma \n",
    "    output['team_xg'] = output['team_xg'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['opp_xg'] = output['opp_xg'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['team_form'] = output['team_form'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['opp_form'] = output['opp_form'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['team_points'] = output['team_points'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['opp_points'] = output['opp_points'].apply(lambda x: str(x).replace('.', ','))\n",
    "    \n",
    "    output['AvgH'] = output['AvgH'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['AvgD'] = output['AvgD'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['AvgA'] = output['AvgA'].apply(lambda x: str(x).replace('.', ','))\n",
    "\n",
    "    output['Prob1'] = output['Prob1'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['ProbX'] = output['ProbX'].apply(lambda x: str(x).replace('.', ','))\n",
    "    output['Prob2'] = output['Prob2'].apply(lambda x: str(x).replace('.', ','))\n",
    "\n",
    "    output['Confidence'] = output['Confidence'].apply(lambda x: str(x).replace('.', ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOTAL ROWS: \", len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp\n",
    "import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "# Format the current date and time as a string\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# save filtered_df_val[display_columns] to a CSV file\n",
    "output.to_csv(f'data/predictions/predictions_{content}_{timestamp}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 400  # Set Frequency To 2500 Hertz\n",
    "duration = 200  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-pyt